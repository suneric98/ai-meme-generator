{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ID                   Name  \\\n",
      "0  112126428   Distracted Boyfriend   \n",
      "1     438680  Batman Slapping Robin   \n",
      "2   87743020            Two Buttons   \n",
      "3  181913649    Drake Hotline Bling   \n",
      "4      61579    One Does Not Simply   \n",
      "\n",
      "                                     Alternate Names  \n",
      "0  distracted bf, guy checking out another girl, ...  \n",
      "1                                                NaN  \n",
      "2  2 red buttons, choice button, which button, da...  \n",
      "3  drakeposting, drakepost, drake hotline approve...  \n",
      "4  one does not simply walk into morder, lord of ...  \n"
     ]
    }
   ],
   "source": [
    "# Basic data files and paths\n",
    "TOP_100_PATH = '../data/popular_100_memes.csv'\n",
    "TOP_100 = pd.read_csv(TOP_100_PATH, encoding = \"ISO-8859-1\")\n",
    "DATA_PATH = '../data/memes/'\n",
    "\n",
    "STATS_PATH = '../data/statistics.json'\n",
    "with open(STATS_PATH, 'r') as f:\n",
    "    STATS = json.load(f)\n",
    "print(TOP_100.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "Bad-Luck-Brian.json 32141\n",
      "One-Does-Not-Simply.json 26186\n",
      "Philosoraptor.json 19971\n",
      "Boardroom-Meeting-Suggestion.json 18164\n",
      "Futurama-Fry.json 17325\n",
      "First-World-Problems.json 17272\n",
      "The-Most-Interesting-Man-In-The-World.json 15850\n",
      "Batman-Slapping-Robin.json 14802\n",
      "Bad-Pun-Dog.json 13194\n",
      "X-X-Everywhere.json 12985\n",
      "Expanding-Brain.json 12826\n",
      "Two-Buttons.json 12232\n",
      "But-Thats-None-Of-My-Business.json 11097\n",
      "Ancient-Aliens.json 10555\n",
      "Creepy-Condescending-Wonka.json 10374\n",
      "Grumpy-Cat.json 9569\n",
      "Y-U-No.json 9370\n",
      "The-Rock-Driving.json 9286\n",
      "That-Would-Be-Great.json 8600\n",
      "Waiting-Skeleton.json 8592\n",
      "Change-My-Mind.json 8563\n",
      "Leonardo-Dicaprio-Cheers.json 8377\n",
      "10-Guy.json 8258\n",
      "Captain-Picard-Facepalm.json 8105\n",
      "Matrix-Morpheus.json 7833\n",
      "Third-World-Skeptical-Kid.json 7782\n",
      "Confession-Bear.json 7778\n",
      "Am-I-The-Only-One-Around-Here.json 7291\n",
      "Success-Kid.json 7074\n",
      "Hide-the-Pain-Harold.json 6973\n",
      "Roll-Safe-Think-About-It.json 6948\n",
      "Evil-Toddler.json 6106\n",
      "Awkward-Moment-Sealion.json 6101\n",
      "Face-You-Make-Robert-Downey-Jr.json 6100\n",
      "Mocking-Spongebob.json 5795\n",
      "Disaster-Girl.json 5742\n",
      "Blank-Nut-Button.json 5714\n",
      "Dont-You-Squidward.json 5599\n",
      "Inhaling-Seagull.json 5539\n",
      "Back-In-My-Day.json 5374\n",
      "Grandma-Finds-The-Internet.json 5160\n",
      "Brace-Yourselves-X-is-Coming.json 5029\n",
      "Conspiracy-Keanu.json 4739\n",
      "Picard-Wtf.json 4640\n",
      "Woman-Yelling-At-Cat.json 4604\n",
      "And-everybody-loses-their-minds.json 4464\n",
      "Ill-Just-Wait-Here.json 4419\n",
      "The-Scroll-Of-Truth.json 4389\n",
      "Left-Exit-12-Off-Ramp.json 4319\n",
      "Unsettled-Tom.json 4101\n",
      "Black-Girl-Wat.json 4010\n",
      "Finding-Neverland.json 3874\n",
      "Third-World-Success-Kid.json 3758\n",
      "Surprised-Pikachu.json 3507\n",
      "Skeptical-Baby.json 3368\n",
      "Doge.json 3323\n",
      "Jack-Sparrow-Being-Chased.json 3313\n",
      "Is-This-A-Pigeon.json 3247\n",
      "Running-Away-Balloon.json 3194\n",
      "X-All-The-Y.json 3119\n",
      "Put-It-Somewhere-Else-Patrick.json 3089\n",
      "Scumbag-Steve.json 2997\n",
      "Too-Damn-High.json 2875\n",
      "Star-Wars-Yoda.json 2753\n",
      "Be-Like-Bill.json 2743\n",
      "Drake-Hotline-Bling.json 2690\n",
      "Tuxedo-Winnie-The-Pooh.json 2558\n",
      "Aaaaand-Its-Gone.json 2557\n",
      "Laughing-Men-In-Suits.json 2515\n",
      "American-Chopper-Argument.json 2468\n",
      "Say-That-Again-I-Dare-You.json 2433\n",
      "Aint-Nobody-Got-Time-For-That.json 2378\n",
      "Evil-Kermit.json 2335\n",
      "Sparta-Leonidas.json 2201\n",
      "Oprah-You-Get-A.json 2137\n",
      "Maury-Lie-Detector.json 2125\n",
      "Dr-Evil-Laser.json 2103\n",
      "Mugatu-So-Hot-Right-Now.json 2006\n",
      "This-Is-Where-Id-Put-My-Trophy-If-I-Had-One.json 1899\n",
      "Trump-Bill-Signing.json 1726\n",
      "Yall-Got-Any-More-Of-That.json 1719\n",
      "See-Nobody-Cares.json 1655\n",
      "Yo-Dawg-Heard-You.json 1520\n",
      "I-Should-Buy-A-Boat-Cat.json 1435\n",
      "Spongebob-Ight-Imma-Head-Out.json 1404\n",
      "Imagination-Spongebob.json 1390\n",
      "Marked-Safe-From.json 1308\n",
      "Steve-Harvey.json 1304\n",
      "Hard-To-Swallow-Pills.json 1135\n",
      "Who-Would-Win.json 1131\n",
      "Who-Killed-Hannibal.json 1008\n",
      "Uncle-Sam.json 1007\n",
      "UNO-Draw-25-Cards.json 800\n",
      "Archer.json 695\n",
      "Epic-Handshake.json 473\n",
      "Look-At-Me.json 392\n",
      "Bernie-I-Am-Once-Again-Asking-For-Your-Support.json 347\n",
      "Monkey-Puppet.json 341\n",
      "Sad-Pablo-Escobar.json 276\n"
     ]
    }
   ],
   "source": [
    "memes_count = Counter(STATS['memes'])\n",
    "print(len(STATS['memes']))\n",
    "for f, c in memes_count.most_common():\n",
    "    print(f, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_memes = [\n",
    "    'Woman-Yelling-At-Cat.json', \n",
    "    'Left-Exit-12-Off-Ramp.json', \n",
    "    'Surprised-Pikachu.json',    \n",
    "    'Is-This-A-Pigeon.json',  \n",
    "    'Drake-Hotline-Bling.json', \n",
    "    'Blank-Nut-Button.json', \n",
    "    'One-Does-Not-Simply.json',\n",
    "    'Change-My-Mind.json', \n",
    "    'Roll-Safe-Think-About-It.json', \n",
    "    'Leonardo-Dicaprio-Cheers.json' \n",
    "]\n",
    "def open_data(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "memes_data = []\n",
    "y = []\n",
    "\n",
    "for i,meme in enumerate(selected_memes):\n",
    "    data = open_data(DATA_PATH + meme)\n",
    "    for j, d in enumerate(data):\n",
    "        if ''.join(d['boxes']).strip() != '':\n",
    "            memes_data.append(d)\n",
    "            y.append(i)\n",
    "            if j > 3000:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://i.imgflip.com/3z10rd.jpg', 'post': 'https://imgflip.com/i/3z10rd', 'metadata': {'views': '366', 'img-votes': '1', 'title': 'What are you doing me?', 'author': 'mjelliott80'}, 'boxes': [\"me: i don't know what to do with my life\", \"me to me: and i don't want to see it\"]}\n",
      "29706\n",
      "29706\n"
     ]
    }
   ],
   "source": [
    "print(memes_data[0])\n",
    "print(len(memes_data))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data for modelling\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# functions to preprocess data into a list of words, blocks separated by |\n",
    "def parse_text(text):\n",
    "    text = text.lower().strip()\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def parse_blocks(blocks):\n",
    "    result = []\n",
    "    for i,b in enumerate(blocks):\n",
    "        result.extend(parse_text(b))\n",
    "        if i < len(blocks) - 1:\n",
    "            result.extend('|')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['me', ':', 'i', 'do', \"n't\", 'know', 'what', 'to', 'do', 'with', 'my', 'life', '|', 'me', 'to', 'me', ':', 'and', 'i', 'do', \"n't\", 'want', 'to', 'see', 'it']\n"
     ]
    }
   ],
   "source": [
    "X_words = [parse_blocks(meme['boxes']) for meme in memes_data]\n",
    "print(X_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text = [' '.join(text) for text in X_words]\n",
    "plt.hist(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 864 candidates, totalling 4320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:   56.7s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1144 tasks      | elapsed: 10.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1560 tasks      | elapsed: 18.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2040 tasks      | elapsed: 22.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2584 tasks      | elapsed: 29.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3192 tasks      | elapsed: 35.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3864 tasks      | elapsed: 47.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4320 out of 4320 | elapsed: 49.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.659\n",
      "Best parameters set:\n",
      "\tclf__C: 1.0\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'lbfgs'\n",
      "\tclf__tol: 1e-05\n",
      "\ttfidf__max_df: 0.5\n",
      "\ttfidf__min_df: 3\n",
      "\ttfidf__ngram_range: (1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericsun/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# taking sample of text to optimize\n",
    "X_sample, _, y_sample, _ = train_test_split(X_text, y, train_size=0.2, test_size=0.8)\n",
    "\n",
    "# optimizing tfidf with pipeline and gridsearch: takes a very very long time to run\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf',TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'tfidf__max_df':(0.5,0.75,1.0),\n",
    "    'tfidf__min_df':(0.2,1,3),\n",
    "    'tfidf__ngram_range':((1,1),(1,2)),\n",
    "    'clf__C': (0.2,0.5,0.8,1.0),\n",
    "    'clf__tol': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    'clf__solver': ('sag','saga','lbfgs')\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=3) #perform gridsearch\n",
    "grid_search.fit(X_sample, y_sample)\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74152, 11731)\n"
     ]
    }
   ],
   "source": [
    "# implementing best results\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=3, ngram_range=(1,1))\n",
    "\n",
    "vectors = vectorizer.fit_transform(X_text)\n",
    "print(vectors.shape)\n",
    "scaled_vectors = StandardScaler().fit_transform(vectors.toarray())\n",
    "\n",
    "model = LogisticRegression(C=1.0,penalty='l2',solver='lbfgs', max_iter=100, tol=0.00001)\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_vectors, y, train_size=0.8, test_size=0.2)\n",
    "model = model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74152, 36022)\n"
     ]
    }
   ],
   "source": [
    "# implementing normal result\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "vectors = vectorizer.fit_transform(X_text)\n",
    "print(vectors.shape)\n",
    "scaled_vectors = StandardScaler().fit_transform(vectors.toarray())\n",
    "\n",
    "model = LogisticRegression()\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_vectors, y, train_size=0.8, test_size=0.2)\n",
    "model = model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pred)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
