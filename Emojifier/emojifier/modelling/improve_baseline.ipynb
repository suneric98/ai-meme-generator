{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import emoji\n",
    "from os.path import join\n",
    "import json\n",
    "import time\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets\n",
      "['üí´ IT‚ÄôS BURGER MONDAY, CBUS! üí´ Order any Gourmet Burger with a draft beer üçîüç∫ ifor just $9.99 when you dine in. üí™ Cer‚Ä¶', 'Stay strong üí™ You are an amazing man. I appreciate the truth that is told.', 'Code Sale Awesome from Styli üí™']\n",
      "122179\n",
      "Chosen emojis\n",
      "['üòÇ', 'üòç', 'üò≠', 'üòä', 'üíï', 'üòí', 'üòâ', 'üëå', 'üëç', 'üôè', 'üëÄ', 'üî•', 'üíØ', 'üëè', 'üí™']\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# getting the twitter comments\n",
    "DATA_PATH = join('..','data','twitter-data-cleaned.txt')\n",
    "with open(DATA_PATH, 'r',  encoding=\"utf-8\") as f:\n",
    "    data = f.readlines()\n",
    "data = [d.strip() for d in data if d.strip() != '']\n",
    "print('Tweets')\n",
    "print(data[:3])\n",
    "print(len(data))\n",
    "\n",
    "# getting our chosen emojis\n",
    "SELECTED_EMOJIS_PATH = join('..','data','best-emojis.json')\n",
    "with open(SELECTED_EMOJIS_PATH, 'r', encoding='utf-8') as f:\n",
    "    EMOJIS = json.load(f)\n",
    "EMOJI_CHARS = [e['char'] for e in EMOJIS]\n",
    "print('Chosen emojis')\n",
    "print(EMOJI_CHARS)\n",
    "print(len(EMOJI_CHARS))\n",
    "\n",
    "ALL_EMOJIS = set(emoji.emojize(emoji_code) for emoji_code in emoji.UNICODE_EMOJI.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üòÇ\n",
      "8440\n",
      "[['go', 'lol', 'postmen', 'familiar', 'face'], ['bollywood', 'reality'], ['got', 'even', 'though', 'debates', 'blasphemy']]\n",
      "üòç\n",
      "7329\n",
      "[['week', 'studio', 'figura', 'start', 'today'], ['200', 'days', 'go'], ['200', 'days', 'go']]\n",
      "üò≠\n",
      "7428\n",
      "[['thank', 'lol', 'procrastination', 'almost'], ['change', 'first', 'diaper', 'uncle', 'year'], ['deposit', '52', 'yuan', 'set', 'must']]\n",
      "üòä\n",
      "6335\n",
      "[['moment', 'today', 'credit', 'stacie', 'swift'], ['fantastic', 'well', 'done', 'ladies'], ['congratulations', 'fam', 'best']]\n",
      "üíï\n",
      "6480\n",
      "[['happy', 'birthday', 'mochiiii', 'saranghae', 'take'], ['got', 'soon', 'youre', 'done'], ['got']]\n",
      "üòí\n",
      "5852\n",
      "[['Ô∏è', 'Ô∏è', 'dunk', 'ang', 'hina'], ['drink', 'comes', '17', 'uber', 'eats'], ['drink', 'comes', '17', 'uber', 'eats']]\n",
      "üòâ\n",
      "5905\n",
      "[['figura', 'start', 'today', 'choose', 'package'], ['someone', 'said', 'richest', 'man', 'age'], ['yall', 'know', 'rest']]\n",
      "üëå\n",
      "6023\n",
      "[['yess'], ['nice', 'one', 'born', 'leader', 'keep'], ['nobody', 'perfect', 'best', 'great', 'artist']]\n",
      "üëç\n",
      "6446\n",
      "[['promises', 'made'], ['88m', 'tweets', 'sarkaruvaaripaata'], ['succeed', 'succeed', 'southstl', 'stlallday', 'stl4life']]\n",
      "üôè\n",
      "6760\n",
      "[['keep', 'going', 'power', 'maam'], ['facts', 'i‚Äôm', 'interested', 'want', 'work'], ['didn‚Äôt', 'read', 'thread', 'tweet', 'well']]\n",
      "üëÄ\n",
      "5835\n",
      "[['post', 'app', 'college', 'coaches', 'fans'], ['live', 'tonight', '7pm', 'mst'], ['u', 'make', '5', 'k', 'tweets']]\n",
      "üî•\n",
      "9913\n",
      "[['wickets', 'broken'], ['nnz'], ['nnz']]\n",
      "üíØ\n",
      "6419\n",
      "[['papa', 'keep', 'bro', 'full', 'support'], ['richest', 'man', 'age', 'didn‚Äôt', 'say'], ['200', 'days', 'go']]\n",
      "üëè\n",
      "9210\n",
      "[['bright', 'future', 'young', 'mans', 'got'], ['„ÄÇ', '„ÄÇ', '„ÄÇ'], ['trump', '2020']]\n",
      "üí™\n",
      "6481\n",
      "[['draft', 'beer', 'ifor', '999', 'dine'], ['stay', 'strong', 'amazing', 'man', 'appreciate'], ['code', 'sale', 'awesome', 'styli']]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing the data\n",
    "from parsing import Tokenizer, TokenType, Token\n",
    "tokenizer = Tokenizer(EMOJI_CHARS)\n",
    "# take 3 previous words as context for the emoji\n",
    "context = {e:[] for e in EMOJI_CHARS}\n",
    "emojiToId = {e:i for i,e in enumerate(EMOJI_CHARS)}\n",
    "\n",
    "for tweet in data:\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    for i,token in enumerate(tokens):\n",
    "        if token.token_type == TokenType.EMOJIS:\n",
    "            closest = tokenizer.findClosestNWords(5, tokens, i)\n",
    "            if closest:\n",
    "                context[token.raw].append(closest)\n",
    "\n",
    "for e, words in context.items():\n",
    "    print(e)\n",
    "    print(len(words))\n",
    "    print(words[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72665\n"
     ]
    }
   ],
   "source": [
    "# making X and y for TFIDF as a baseline idea of how good our accuracy can expect to be\n",
    "X_words = []\n",
    "y = []\n",
    "sentences = set()\n",
    "for e, words in context.items():\n",
    "    for i,word_list in enumerate(words):\n",
    "        sentence = ' '.join(word_list)\n",
    "        if sentence in sentences:\n",
    "            continue\n",
    "        sentences.add(sentence)\n",
    "        X_words.append(sentence)\n",
    "        y.append(emojiToId[e])\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.20 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "# attempting to use tfidf and RF\n",
    "# WARNING: can take a decent amount of time: 5 mins or so?\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(X_words)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.18 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "# attempting to use count_vectorizer and RF\n",
    "# WARNING: can take a decent amount of time: 5 mins or so?\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(X_words)\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN building to predict result\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go', 'lol', 'postmen', 'familiar', 'face']\n",
      "72665\n"
     ]
    }
   ],
   "source": [
    "# making X and y for RNN\n",
    "X_words = []\n",
    "y = []\n",
    "sentences = set()\n",
    "for e, words in context.items():\n",
    "    for i, sentence in enumerate(words):\n",
    "        check = ' '.join(sentence)\n",
    "        if check in sentences:\n",
    "            continue\n",
    "        sentences.add(check)\n",
    "        X_words.append(sentence)\n",
    "        y.append(emojiToId[e])\n",
    "print(X_words[0])\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making word embeddings for RNN\n",
    "UNK = \"<unk>\"\n",
    "WORDEMBSIZE = 100\n",
    "W2V_WINDOW = 7\n",
    "W2V_COUNT = 1\n",
    "W2V_EPOCH=100\n",
    "\n",
    "def makeVocab(text):\n",
    "    vocab = set()\n",
    "    vocab.add(UNK)\n",
    "    for sentences in text:\n",
    "        for word in sentences:\n",
    "            vocab.add(word)\n",
    "    return vocab\n",
    "\n",
    "def makeEmbModel(data):\n",
    "#     model = FastText(data, size=WORDEMBSIZE, window=3, min_count=1, iter=10, sorted_vocab=1)\n",
    "    model = Word2Vec(window=W2V_WINDOW, min_count=W2V_COUNT, size=WORDEMBSIZE)\n",
    "    model.build_vocab(data)\n",
    "    model.train(data, total_examples=len(data), epochs=W2V_EPOCH)\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "def makeEmbeddings(data, model, vocab):\n",
    "    vecData = []\n",
    "    for sentence in data:\n",
    "        wordEmbs = []\n",
    "        for word in sentence:\n",
    "            if word in vocab:\n",
    "#                 print(type(model[word]))\n",
    "#                 print(model[word])\n",
    "                wordEmbs.append(model.wv[word])\n",
    "            else:\n",
    "                wordEmbs.append(np.zeros(WORDEMBSIZE))\n",
    "        wordEmbs = torch.FloatTensor(wordEmbs)\n",
    "        vecData.append(wordEmbs)\n",
    "    return vecData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=32885, size=100, alpha=0.025)\n",
      "['llores', 'bu']\n",
      "tensor([[ 0.0726,  0.3071,  0.1215,  0.1497,  0.0015, -0.1999, -0.0821, -0.0113,\n",
      "         -0.1436, -0.2638, -0.0035, -0.2923, -0.0427,  0.2550,  0.0135,  0.0641,\n",
      "          0.2646, -0.1737,  0.0140, -0.1118,  0.0473,  0.1453,  0.0037, -0.1263,\n",
      "         -0.0384, -0.0687, -0.0493, -0.1431, -0.1057, -0.1498, -0.1081,  0.0979,\n",
      "          0.0348, -0.0672, -0.0906,  0.2044, -0.0427,  0.0794,  0.1522, -0.3557,\n",
      "          0.0892, -0.0883, -0.0411,  0.0468, -0.0293,  0.0626, -0.0558, -0.0076,\n",
      "          0.2377,  0.0057, -0.1611, -0.0549, -0.1242, -0.0042,  0.0562, -0.1509,\n",
      "          0.0740, -0.1027,  0.2137, -0.1154, -0.0855, -0.0365,  0.1870, -0.0524,\n",
      "         -0.1420,  0.0887, -0.0350,  0.2134,  0.1544,  0.1406,  0.0346,  0.0072,\n",
      "         -0.0298,  0.0851,  0.1626, -0.1545,  0.0430,  0.0725, -0.1892, -0.1618,\n",
      "         -0.0782, -0.1726,  0.0180,  0.0380,  0.1582,  0.0925, -0.0155, -0.0879,\n",
      "         -0.1873,  0.0706,  0.0046, -0.2998, -0.1512, -0.0173, -0.0059,  0.0212,\n",
      "         -0.2911, -0.4377, -0.1142,  0.1606],\n",
      "        [ 0.0442,  0.9852, -0.1690,  0.0817, -0.2802, -0.6475, -0.0079,  0.0330,\n",
      "         -0.6479, -0.4054,  0.2197, -0.6496,  0.1175,  0.1928,  0.1776, -0.0633,\n",
      "          0.5406, -0.5044,  0.2981, -0.1534,  0.2158,  0.4678,  0.1485, -0.5734,\n",
      "          0.2074, -0.0925, -0.3255, -0.5441, -0.5063, -0.2937, -0.0902,  0.0045,\n",
      "          0.0702, -0.1086, -0.5848,  0.3545, -0.0501, -0.1493,  0.1906, -0.6944,\n",
      "         -0.0746,  0.3893, -0.0540,  0.1737, -0.0532,  0.0336, -0.0454, -0.3668,\n",
      "          0.6496, -0.4097, -0.9125,  0.1822, -0.3314,  0.0414,  0.2191, -0.5116,\n",
      "         -0.0116, -0.7250,  0.3181, -0.2838, -0.2029, -0.2808,  0.2864, -0.5116,\n",
      "         -0.3404,  0.1113,  0.1591,  0.3402,  0.2995,  0.4447, -0.1932,  0.2365,\n",
      "         -0.1987,  0.3079,  0.0545, -0.1519,  0.5267, -0.0154, -0.7325, -0.4635,\n",
      "          0.1829, -0.5798,  0.4545,  0.2598,  0.6432,  0.2277,  0.0738, -0.7433,\n",
      "         -0.2030,  0.0387,  0.1353, -0.8454, -0.4110, -0.1063, -0.1024, -0.1603,\n",
      "         -0.6804, -1.3555, -0.1024,  0.3702]])\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# continue making word embeddings\n",
    "X_words_train, X_words_test, y_train, y_test = train_test_split(X_words, y, train_size=0.8, test_size=0.2, random_state=42)\n",
    "vocab = makeVocab(X_words_train)\n",
    "emb_model = makeEmbModel(X_words_train)\n",
    "vocab = set(list(emb_model.wv.vocab.keys()))\n",
    "X_train = makeEmbeddings(X_words_train, emb_model, vocab)\n",
    "X_test = makeEmbeddings(X_words_test, emb_model, vocab)\n",
    "\n",
    "print(X_words_train[0])\n",
    "print(X_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, h, output_dim = 15):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_dim, h, dropout=0.2)\n",
    "        self.finalLayer = nn.Linear(h, output_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = h\n",
    "        self.output_dim =  output_dim\n",
    "#         self.word_embeddings = nn.Embedding(vocab_size, input_dim)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        self.loss = nn.NLLLoss()\n",
    "    \n",
    "    def compute_loss(self, predicted_vector, gold_label):\n",
    "        return self.loss(predicted_vector, gold_label)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        out, hidden = self.rnn(inputs)\n",
    "        hidden = hidden.contiguous().view(-1,self.hidden_dim)\n",
    "        predicted_vector = self.softmax(self.finalLayer(hidden))\n",
    "        return predicted_vector\n",
    "    \n",
    "class biRNN(nn.Module):\n",
    "    def __init__(self, input_dim, h, output_dim = 15):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_dim, h, dropout=0.2, bidirectional=True)\n",
    "        self.finalLayer = nn.Linear(h * 2, output_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = h\n",
    "        self.output_dim =  output_dim\n",
    "#         self.word_embeddings = nn.Embedding(vocab_size, input_dim)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        self.loss = nn.NLLLoss()\n",
    "    \n",
    "    def compute_loss(self, predicted_vector, gold_label):\n",
    "        return self.loss(predicted_vector, gold_label)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        out, hidden = self.rnn(inputs)\n",
    "        hidden = hidden.contiguous().view(-1,self.hidden_dim)\n",
    "        predicted_vector = self.softmax(self.finalLayer(hidden))\n",
    "        return predicted_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------------\n",
      "EPOCH: 1\n",
      "Training started for epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5be88d1a50421aa5fae814e2300dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=908.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericsun/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed for epoch: 1\n",
      "Time for train: 91.91886281967163\n",
      "Accuracy: 0.11730795704845814 Loss: 2.649101972579956\n",
      "Validation started for epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c30b02da09d4bdbaf196943ce171a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=227.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation completed for epoch: 1\n",
      "Time for validation: 8.825459241867065\n",
      "Accuracy: 0.14743942731277532 Loss: 2.5859882831573486\n",
      "\n",
      "\n",
      "-------------\n",
      "EPOCH: 2\n",
      "Training started for epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66072d22406d409cb3ca0a332b6e2c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=908.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed for epoch: 2\n",
      "Time for train: 75.3097620010376\n",
      "Accuracy: 0.16611026982378854 Loss: 2.534013271331787\n",
      "Validation started for epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a216b946e140bfbcb02e70a9dd8575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=227.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation completed for epoch: 2\n",
      "Time for validation: 8.72238302230835\n",
      "Accuracy: 0.17084251101321585 Loss: 2.5054945945739746\n",
      "\n",
      "\n",
      "-------------\n",
      "EPOCH: 3\n",
      "Training started for epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "118605e555414ed8851c36be03cfb1a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=908.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed for epoch: 3\n",
      "Time for train: 74.24760603904724\n",
      "Accuracy: 0.18357654185022027 Loss: 2.4835872650146484\n",
      "Validation started for epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb3b8233efc4ce89a45258fc5ccb342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=227.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation completed for epoch: 3\n",
      "Time for validation: 8.519360065460205\n",
      "Accuracy: 0.1823375550660793 Loss: 2.47572660446167\n",
      "\n",
      "\n",
      "-------------\n",
      "EPOCH: 4\n",
      "Training started for epoch: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42548e61d6ed4b54b8024e23ed611f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=908.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed for epoch: 4\n",
      "Time for train: 74.639328956604\n",
      "Accuracy: 0.19085558920704845 Loss: 2.461818218231201\n",
      "Validation started for epoch: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88fe4209579b4375b79162a4151a02d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=227.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation completed for epoch: 4\n",
      "Time for validation: 8.735416889190674\n",
      "Accuracy: 0.18626101321585903 Loss: 2.4600400924682617\n",
      "\n",
      "\n",
      "-------------\n",
      "EPOCH: 5\n",
      "Training started for epoch: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119755b667bf4656aec5588c570f5ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=908.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed for epoch: 5\n",
      "Time for train: 79.23219704627991\n",
      "Accuracy: 0.19429721916299558 Loss: 2.450086832046509\n",
      "Validation started for epoch: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92fe3be12a1446e88a3d701e697f8e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=227.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation completed for epoch: 5\n",
      "Time for validation: 10.038591861724854\n",
      "Accuracy: 0.18915198237885464 Loss: 2.451871156692505\n",
      "\n",
      "\n",
      "-------------\n",
      "EPOCH: 6\n",
      "Training started for epoch: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9b83c054204da297310f096710ede3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=908.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed for epoch: 6\n",
      "Time for train: 90.93724489212036\n",
      "Accuracy: 0.19789372246696035 Loss: 2.4431374073028564\n",
      "Validation started for epoch: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee1442467b24c1686330380ada6b086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=227.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation completed for epoch: 6\n",
      "Time for validation: 8.413643836975098\n",
      "Accuracy: 0.19114812775330398 Loss: 2.4468648433685303\n",
      "\n",
      "\n",
      "-------------\n",
      "EPOCH: 7\n",
      "Training started for epoch: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d78f868ef9a84ea1973761b8f5900345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=908.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed for epoch: 7\n",
      "Time for train: 76.35072803497314\n",
      "Accuracy: 0.19882296255506607 Loss: 2.438063144683838\n",
      "Validation started for epoch: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8026a77bc7ad42198ccb666fee4d3e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=227.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation completed for epoch: 7\n",
      "Time for validation: 8.796296834945679\n",
      "Accuracy: 0.19094162995594713 Loss: 2.444852828979492\n",
      "\n",
      "\n",
      "-------------\n",
      "EPOCH: 8\n",
      "Training started for epoch: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b0cd2e87ca41b1af8f0605b808886d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=908.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed for epoch: 8\n",
      "Time for train: 75.76262187957764\n",
      "Accuracy: 0.20044052863436124 Loss: 2.4353044033050537\n",
      "Validation started for epoch: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a584a69c63444f907b0338aa7c3bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=227.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation completed for epoch: 8\n",
      "Time for validation: 8.891098260879517\n",
      "Accuracy: 0.19128579295154186 Loss: 2.441119432449341\n",
      "\n",
      "\n",
      "-------------\n",
      "EPOCH: 9\n",
      "Training started for epoch: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410b161ecd4f442681c9a27e247034bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=908.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-ae91a1b5d521>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# running epochs for training and validation\n",
    "HIDDEN_DIM = 124\n",
    "EPOCHS = 15\n",
    "minibatch_size = 64\n",
    "\n",
    "model = RNN(WORDEMBSIZE, HIDDEN_DIM, 15)\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.01, weight_decay=0.01)\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"\\n\\n-------------\")\n",
    "    print(\"EPOCH: {}\".format(epoch + 1))\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    print(\"Training started for epoch: {}\".format(epoch + 1))\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    start_time = time.time()\n",
    "    correct = total = 0\n",
    "    N = len(y_train)\n",
    "    for minibatch_idx in tqdm(range(N // minibatch_size)):\n",
    "        optimizer.zero_grad()\n",
    "        loss = None\n",
    "        for idx in range(minibatch_size):\n",
    "            text = X_train[minibatch_idx * minibatch_size + idx]\n",
    "            text = torch.unsqueeze(text, 1)\n",
    "            labelIdx = y_train[minibatch_idx * minibatch_size + idx]\n",
    "            log_probs = model(text)\n",
    "            text_loss = model.compute_loss(log_probs.view(1,-1), torch.tensor([labelIdx]))\n",
    "            running_loss += text_loss\n",
    "            if loss is None:\n",
    "                loss = text_loss\n",
    "            else:\n",
    "                loss += text_loss\n",
    "            pred_label = torch.argmax(log_probs)\n",
    "            correct += int(pred_label == labelIdx)\n",
    "            total += 1\n",
    "        loss = loss / minibatch_size\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss.append(running_loss / N)\n",
    "    train_acc.append(correct / total)\n",
    "    print(\"Training completed for epoch: {}\".format(epoch + 1))\n",
    "    print(\"Time for train: {}\".format(time.time() - start_time))\n",
    "    print(\"Accuracy: {} Loss: {}\".format(correct / total, train_loss[-1]))\n",
    "    \n",
    "    #validation\n",
    "    running_loss = 0.0\n",
    "    model.eval()\n",
    "    optimizer.zero_grad()\n",
    "    print(\"Validation started for epoch: {}\".format(epoch + 1))\n",
    "    X_test, y_test = shuffle(X_test, y_test)\n",
    "    start_time = time.time()\n",
    "    correct = total = 0\n",
    "    N = len(y_test)\n",
    "    for minibatch_idx in tqdm(range(N // minibatch_size)):\n",
    "        optimizer.zero_grad()\n",
    "        for idx in range(minibatch_size):\n",
    "            text = X_test[minibatch_idx * minibatch_size + idx]\n",
    "            text = torch.unsqueeze(text, 1)\n",
    "            labelIdx = y_test[minibatch_idx * minibatch_size + idx]\n",
    "            log_probs = model(text)\n",
    "            text_loss = model.compute_loss(log_probs.view(1,-1), torch.tensor([labelIdx]))\n",
    "            running_loss += text_loss\n",
    "            pred_label = torch.argmax(log_probs)\n",
    "            correct += int(pred_label == labelIdx)\n",
    "            total += 1\n",
    "    curr_loss = running_loss / N\n",
    "    test_loss.append(curr_loss)\n",
    "    test_acc.append(correct / total)\n",
    "    print(\"Validation completed for epoch: {}\".format(epoch + 1))\n",
    "    print(\"Time for validation: {}\".format(time.time() - start_time))\n",
    "    print(\"Accuracy: {} Loss: {}\".format(correct / total, curr_loss))\n",
    "    if len(test_loss > 3) and curr_loss >= test_loss[-3]:\n",
    "        print(\"Stopping progress: no longer learning for validation\")\n",
    "        break\n",
    "\n",
    "plt.plot(train_loss, label='train loss')\n",
    "plt.plot(test_loss, label='test loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_acc, label='train acc')\n",
    "plt.plot(test_acc, label='test acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
