{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import emoji\n",
    "from os.path import join\n",
    "import json\n",
    "import time\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/twitter-data-cleaned.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f931be28c182>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# getting the twitter comments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mDATA_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'..'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'twitter-data-cleaned.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/twitter-data-cleaned.txt'"
     ]
    }
   ],
   "source": [
    "# getting the twitter comments\n",
    "DATA_PATH = join('..','data','twitter-data-cleaned.txt')\n",
    "with open(DATA_PATH, 'r',  encoding=\"utf-8\") as f:\n",
    "    data = f.readlines()\n",
    "data = [d.strip() for d in data if d.strip() != '']\n",
    "print('Tweets')\n",
    "print(data[:3])\n",
    "print(len(data))\n",
    "\n",
    "# getting our chosen emojis\n",
    "SELECTED_EMOJIS_PATH = join('..','data','best-emojis.json')\n",
    "with open(SELECTED_EMOJIS_PATH, 'r', encoding='utf-8') as f:\n",
    "    EMOJIS = json.load(f)\n",
    "EMOJI_CHARS = [e['char'] for e in EMOJIS]\n",
    "print('Chosen emojis')\n",
    "print(EMOJI_CHARS)\n",
    "print(len(EMOJI_CHARS))\n",
    "\n",
    "ALL_EMOJIS = set(emoji.emojize(emoji_code) for emoji_code in emoji.UNICODE_EMOJI.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üòÇ\n",
      "8440\n",
      "[['yayyy', 'go', 'go', 'lol', 'postmen', 'familiar', 'face', 'time'], ['bollywood', 'reality'], ['brother', 'know', 'always', 'got', 'even', 'though', 'debates', 'blasphemy']]\n",
      "üòç\n",
      "7329\n",
      "[['fitnes', 'week', 'studio', 'figura', 'start', 'today', 'choose', 'package'], ['200', 'days', 'go'], ['200', 'days', 'go']]\n",
      "üò≠\n",
      "7428\n",
      "[['thank', 'lol', 'procrastination', 'almost'], ['change', 'first', 'diaper', 'uncle', 'year'], ['deposit', '52', 'yuan', 'set', 'must', 'buy']]\n",
      "üòä\n",
      "6335\n",
      "[['take', 'moment', 'today', 'credit', 'stacie', 'swift', 'edutwitter', 'teachertwitter'], ['fantastic', 'well', 'done', 'ladies'], ['congratulations', 'fam', 'best']]\n",
      "üíï\n",
      "6480\n",
      "[['happy', 'birthday', 'mochiiii', 'saranghae', 'take', 'care', 'self', 'see'], ['got', 'soon', 'youre', 'done'], ['got']]\n",
      "üòí\n",
      "5852\n",
      "[['dunk', 'jonathan', 'Ô∏è', 'Ô∏è', 'Ô∏è', 'dunk', 'ang', 'hina'], ['starbucks', 'however', 'one', 'drink', 'comes', '17', 'uber', 'eats'], ['starbucks', 'however', 'one', 'drink', 'comes', '17', 'uber', 'eats']]\n",
      "üòâ\n",
      "5905\n",
      "[['fitnes', 'week', 'studio', 'figura', 'start', 'today', 'choose', 'package'], ['someone', 'said', 'richest', 'man', 'age', 'didn‚Äôt', 'say'], ['yall', 'know', 'rest']]\n",
      "üëå\n",
      "6023\n",
      "[['yess'], ['nice', 'one', 'born', 'leader', 'keep', 'hail'], ['youre', 'perfect', 'nobody', 'perfect', 'best', 'great', 'artist', 'dont']]\n",
      "üëç\n",
      "6446\n",
      "[['promises', 'made'], ['88m', 'tweets', 'sarkaruvaaripaata'], ['succeed', 'succeed', 'southstl', 'stlallday', 'stl4life']]\n",
      "üôè\n",
      "6760\n",
      "[['keep', 'going', 'power', 'maam'], ['facts', 'i‚Äôm', 'interested', 'want', 'work', 'closer', 'vocalist'], ['yeah', 'sorry', 'angry', 'didn‚Äôt', 'read', 'thread', 'tweet', 'well']]\n",
      "üëÄ\n",
      "5835\n",
      "[['post', 'app', 'college', 'coaches', 'fans', 'watch', 'ball', 'video'], ['live', 'tonight', '7pm', 'mst'], ['u', 'make', '5', 'k', 'tweets']]\n",
      "üî•\n",
      "9913\n",
      "[['wickets', 'broken'], ['nnz'], ['nnz']]\n",
      "üíØ\n",
      "6419\n",
      "[['u', 'strong', 'one', 'papa', 'keep', 'bro', 'full', 'support'], ['someone', 'said', 'richest', 'man', 'age', 'didn‚Äôt', 'say'], ['200', 'days', 'go']]\n",
      "üëè\n",
      "9210\n",
      "[['bright', 'future', 'young', 'mans', 'got'], ['„ÄÇ', '„ÄÇ', '„ÄÇ'], ['trump', '2020']]\n",
      "üí™\n",
      "6481\n",
      "[['order', 'gourmet', 'burger', 'draft', 'beer', 'ifor', '999', 'dine'], ['stay', 'strong', 'amazing', 'man', 'appreciate', 'truth', 'told'], ['code', 'sale', 'awesome', 'styli']]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing the data\n",
    "from parsing import Tokenizer, TokenType, Token\n",
    "tokenizer = Tokenizer(EMOJI_CHARS)\n",
    "# take 3 previous words as context for the emoji\n",
    "context = {e:[] for e in EMOJI_CHARS}\n",
    "emojiToId = {e:i for i,e in enumerate(EMOJI_CHARS)}\n",
    "\n",
    "for tweet in data:\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    for i,token in enumerate(tokens):\n",
    "        if token.token_type == TokenType.EMOJIS:\n",
    "            closest = tokenizer.findClosestNWords(8, tokens, i)\n",
    "            if closest:\n",
    "                context[token.raw].append(closest)\n",
    "\n",
    "for e, words in context.items():\n",
    "    print(e)\n",
    "    print(len(words))\n",
    "    print(words[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN building to predict result\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yayyy', 'go', 'go', 'lol', 'postmen', 'familiar', 'face', 'time']\n",
      "71857\n"
     ]
    }
   ],
   "source": [
    "# making X and y for RNN\n",
    "X_words = []\n",
    "y = []\n",
    "sentences = set()\n",
    "for e, words in context.items():\n",
    "    for i, sentence in enumerate(words):\n",
    "        check = ' '.join(sentence)\n",
    "        if check in sentences:\n",
    "            continue\n",
    "        sentences.add(check)\n",
    "        X_words.append(sentence)\n",
    "        y.append(emojiToId[e])\n",
    "print(X_words[0])\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making word embeddings for RNN\n",
    "UNK = \"<unk>\"\n",
    "WORDEMBSIZE = 100\n",
    "W2V_WINDOW = 7\n",
    "W2V_COUNT = 1\n",
    "W2V_EPOCH=100\n",
    "\n",
    "def makeVocab(text):\n",
    "    vocab = set()\n",
    "    vocab.add(UNK)\n",
    "    for sentences in text:\n",
    "        for word in sentences:\n",
    "            vocab.add(word)\n",
    "    return vocab\n",
    "\n",
    "def makeEmbModel(data):\n",
    "#     model = FastText(data, size=WORDEMBSIZE, window=3, min_count=1, iter=10, sorted_vocab=1)\n",
    "    model = Word2Vec(window=W2V_WINDOW, min_count=W2V_COUNT, size=WORDEMBSIZE)\n",
    "    model.build_vocab(data)\n",
    "    model.train(data, total_examples=len(data), epochs=W2V_EPOCH)\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "def makeEmbeddings(data, model, vocab):\n",
    "    vecData = []\n",
    "    for sentence in data:\n",
    "        wordEmbs = []\n",
    "        for word in sentence:\n",
    "            if word in vocab:\n",
    "#                 print(type(model[word]))\n",
    "#                 print(model[word])\n",
    "                wordEmbs.append(model.wv[word])\n",
    "            else:\n",
    "                wordEmbs.append(np.zeros(WORDEMBSIZE))\n",
    "        wordEmbs = torch.FloatTensor(wordEmbs)\n",
    "        vecData.append(wordEmbs)\n",
    "    return vecData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=37159, size=100, alpha=0.025)\n",
      "['vdb', 'mctominay', 'abeg']\n",
      "tensor([[ 4.3793e-01,  1.6831e-01,  6.2822e-01, -3.6764e-01, -1.6899e-01,\n",
      "         -1.3530e-01,  1.0859e-01,  4.3041e-01, -1.4167e+00, -8.5227e-02,\n",
      "         -1.7068e-01, -9.7426e-02,  9.0586e-01,  1.0322e-01,  1.1119e+00,\n",
      "          2.9117e-01,  5.5843e-01, -6.5334e-01, -4.6940e-03, -1.4686e-01,\n",
      "          3.5751e-01, -5.1219e-01, -2.3945e-01,  1.8382e-01, -7.0136e-01,\n",
      "          1.3294e+00,  3.3104e-01, -6.6811e-02,  8.7594e-01,  1.5579e-01,\n",
      "         -4.4400e-01,  1.8207e-01, -6.8612e-01, -9.0043e-02, -1.6902e-01,\n",
      "         -1.9993e-02, -8.2884e-01,  4.2203e-01, -4.3094e-01, -4.0533e-01,\n",
      "         -4.9470e-01,  3.4956e-04,  1.3146e+00,  4.3590e-01, -3.5004e-01,\n",
      "          1.0015e-01, -5.5199e-01, -9.9790e-01, -2.7704e-01,  5.5739e-01,\n",
      "          3.2614e-01,  5.1592e-01, -5.5762e-01, -4.8401e-01, -5.5807e-01,\n",
      "         -2.3866e-01, -3.6170e-01, -1.3111e-01,  3.1320e-01,  6.3173e-01,\n",
      "         -6.3873e-02, -3.1641e-01,  6.7442e-01,  2.7336e-01,  1.2302e+00,\n",
      "          9.5187e-01, -4.5582e-01,  1.2095e+00, -4.0260e-01,  1.8267e-01,\n",
      "          7.3514e-01, -4.1877e-01,  9.1628e-01,  3.6128e-01, -2.4541e-01,\n",
      "         -8.4031e-01, -4.5924e-01,  2.4083e-01, -2.9632e-01,  5.9155e-02,\n",
      "         -1.1675e-01, -6.0721e-01, -1.0363e-01,  3.3926e-01, -2.5765e-01,\n",
      "          1.2163e-01, -8.4127e-01,  7.7821e-01,  2.8098e-01, -8.2809e-03,\n",
      "         -4.8466e-01,  3.2230e-01, -1.4651e-01, -2.1842e-01, -5.3069e-01,\n",
      "         -8.1555e-01, -2.7696e-01,  8.3556e-02, -7.7365e-01,  6.8156e-01],\n",
      "        [-5.0532e-01,  2.6833e-01,  1.0448e+00,  4.3993e-01,  2.5746e-01,\n",
      "          7.2037e-01,  4.9904e-01,  1.4634e-02, -6.2446e-01,  1.4638e-01,\n",
      "          3.4494e-01, -5.0280e-01,  5.9397e-01, -1.0654e+00,  1.4345e-01,\n",
      "         -3.5766e-01, -8.7514e-01, -7.6411e-01,  7.0075e-02,  1.4469e+00,\n",
      "          8.8284e-02, -4.2121e-01,  3.2207e-02,  3.9482e-01, -1.2227e-01,\n",
      "          4.1009e-02,  1.1820e+00, -2.7369e-01,  5.7864e-01, -3.4440e-01,\n",
      "         -1.1601e+00, -9.2103e-01, -1.8092e-01, -1.2704e-01, -1.8103e+00,\n",
      "          8.5449e-02, -6.8793e-01,  5.1870e-01,  4.4408e-02, -1.2101e+00,\n",
      "         -5.9792e-01,  7.7405e-01,  1.1927e+00,  2.9005e-01, -9.3879e-01,\n",
      "         -8.4884e-01,  4.2656e-01, -1.2398e+00,  4.1490e-01,  3.9704e-01,\n",
      "          6.2597e-01, -3.2825e-01, -1.4447e-01, -3.8593e-02, -1.0886e+00,\n",
      "          6.9175e-01, -5.8366e-01, -1.7261e-01, -6.2498e-01, -5.8132e-01,\n",
      "          4.9568e-01,  1.3961e-01,  3.9673e-02,  7.7801e-01,  6.8152e-01,\n",
      "          1.3040e+00,  6.8113e-02, -4.3242e-01, -1.8015e-01,  1.7225e-01,\n",
      "          7.8591e-01, -1.0978e+00,  6.2639e-01,  1.7556e-01, -6.2917e-01,\n",
      "         -6.5312e-01, -1.4693e+00,  8.7305e-01,  8.3151e-01,  7.0644e-01,\n",
      "         -1.2947e+00, -1.4914e+00, -1.5817e-01,  1.1882e-01, -2.9133e-02,\n",
      "         -4.6540e-01, -1.0236e+00,  8.2439e-01,  8.8888e-01,  3.7950e-01,\n",
      "          3.6321e-02,  1.3639e-01, -5.4185e-01, -4.4651e-01,  1.1401e-01,\n",
      "         -7.6614e-01,  4.9567e-01, -1.2543e+00, -6.8494e-01, -8.3696e-01],\n",
      "        [-9.9607e-01, -4.9299e-01, -6.5056e-01,  7.9810e-01, -6.4006e-01,\n",
      "          9.9908e-01, -9.1168e-01,  3.7903e-01, -8.9308e-01,  7.3188e-01,\n",
      "         -2.1513e-01, -5.0610e-01,  7.8063e-01, -1.6454e-01,  1.1787e+00,\n",
      "         -1.5177e-01,  7.6465e-01, -3.3264e-01, -1.2543e+00,  7.2345e-01,\n",
      "          6.6926e-01,  6.0660e-01, -2.8268e+00,  2.3835e-01, -5.9168e-01,\n",
      "          1.0081e+00,  7.3808e-01, -2.3742e-01,  1.4499e-01, -1.4533e+00,\n",
      "         -5.2068e-01,  1.5604e+00, -1.4997e+00,  2.2497e-01,  3.7747e-01,\n",
      "         -1.1696e+00,  4.4296e-01,  7.1041e-01, -2.2429e-01,  1.4342e+00,\n",
      "         -4.1567e-01,  1.7292e-01,  5.1093e-01,  5.5929e-03,  6.7613e-01,\n",
      "         -1.4793e+00, -1.1824e+00, -4.2845e-01,  1.7169e-01,  1.9913e+00,\n",
      "         -7.6414e-01, -4.2582e-02,  3.3439e-01,  1.0917e+00,  3.5486e-01,\n",
      "          1.3886e+00, -1.5411e-01, -9.8796e-02,  8.9864e-01,  7.9139e-01,\n",
      "          2.3772e-01, -1.9774e-01,  1.3787e+00,  1.2591e+00, -1.1043e+00,\n",
      "          3.3110e-01, -9.7551e-01, -4.3139e-01,  1.6324e+00,  6.4162e-01,\n",
      "          8.2164e-02, -3.9486e-01, -1.0866e+00,  3.0810e-01, -2.0756e+00,\n",
      "         -1.1846e+00,  7.6458e-01, -6.4871e-01,  1.4544e+00,  5.3830e-01,\n",
      "         -6.3818e-02, -5.8520e-01,  2.0801e+00,  2.0894e-01,  2.5483e-01,\n",
      "         -9.8857e-01, -1.9879e+00,  1.7077e+00,  1.5240e+00,  3.7137e-01,\n",
      "         -2.4891e-02,  1.3749e-01, -1.5198e-01, -8.7493e-01,  7.6029e-01,\n",
      "          2.1532e-01, -3.0522e-02, -1.2779e+00, -1.4407e+00,  2.3746e-01]])\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# continue making word embeddings\n",
    "X_words_train, X_words_test, y_train, y_test = train_test_split(X_words, y, train_size=0.8, test_size=0.2, random_state=42)\n",
    "vocab = makeVocab(X_words_train)\n",
    "emb_model = makeEmbModel(X_words_train)\n",
    "vocab = set(list(emb_model.wv.vocab.keys()))\n",
    "X_train = makeEmbeddings(X_words_train, emb_model, vocab)\n",
    "X_test = makeEmbeddings(X_words_test, emb_model, vocab)\n",
    "\n",
    "print(X_words_train[0])\n",
    "print(X_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, h, output_dim = 15):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_dim, h, dropout=0.2)\n",
    "        self.finalLayer = nn.Linear(h, output_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = h\n",
    "        self.output_dim =  output_dim\n",
    "#         self.word_embeddings = nn.Embedding(vocab_size, input_dim)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        self.loss = nn.NLLLoss()\n",
    "    \n",
    "    def compute_loss(self, predicted_vector, gold_label):\n",
    "        return self.loss(predicted_vector, gold_label)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        out, hidden = self.rnn(inputs)\n",
    "        hidden = hidden.contiguous().view(-1,self.hidden_dim)\n",
    "        predicted_vector = self.softmax(self.finalLayer(hidden))\n",
    "        return predicted_vector\n",
    "    \n",
    "class biRNN(nn.Module):\n",
    "    def __init__(self, input_dim, h, output_dim = 15):\n",
    "        super(biRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_dim, h, dropout=0.2, bidirectional=True)\n",
    "        self.finalLayer = nn.Linear(h, output_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = h\n",
    "        self.output_dim =  output_dim\n",
    "#         self.word_embeddings = nn.Embedding(vocab_size, input_dim)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        self.loss = nn.NLLLoss()\n",
    "    \n",
    "    def compute_loss(self, predicted_vector, gold_label):\n",
    "        return self.loss(predicted_vector, gold_label)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        out, hidden = self.rnn(inputs)\n",
    "        hidden = hidden.contiguous().view(-1,self.hidden_dim)\n",
    "        predicted_vector = self.softmax(self.finalLayer(hidden))\n",
    "        return predicted_vector\n",
    "    \n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_dim, h, output_dim = 15):\n",
    "        super(GRU, self).__init__()\n",
    "        self.rnn = nn.GRU(input_dim, h, dropout=0.2)\n",
    "        self.finalLayer = nn.Linear(h, output_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = h\n",
    "        self.output_dim =  output_dim\n",
    "#         self.word_embeddings = nn.Embedding(vocab_size, input_dim)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        self.loss = nn.NLLLoss()\n",
    "    \n",
    "    def compute_loss(self, predicted_vector, gold_label):\n",
    "        return self.loss(predicted_vector, gold_label)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        out, hidden = self.rnn(inputs)\n",
    "        hidden = hidden.contiguous().view(-1,self.hidden_dim)\n",
    "        predicted_vector = self.softmax(self.finalLayer(hidden))\n",
    "        return predicted_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GRU' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5a7ff118d749>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# model = RNN(WORDEMBSIZE, HIDDEN_DIM, 15)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# model = biRNN(WORDEMBSIZE, HIDDEN_DIM, 15)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWORDEMBSIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIDDEN_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GRU' is not defined"
     ]
    }
   ],
   "source": [
    "# running epochs for training and validation\n",
    "HIDDEN_DIM = 124\n",
    "EPOCHS = 15\n",
    "minibatch_size = 64\n",
    "\n",
    "# model = RNN(WORDEMBSIZE, HIDDEN_DIM, 15)\n",
    "# model = biRNN(WORDEMBSIZE, HIDDEN_DIM, 15)\n",
    "model = GRU(WORDEMBSIZE, HIDDEN_DIM, 15)\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.01)\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"\\n\\n-------------\")\n",
    "    print(\"EPOCH: {}\".format(epoch + 1))\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    print(\"Training started for epoch: {}\".format(epoch + 1))\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    start_time = time.time()\n",
    "    correct = total = 0\n",
    "    N = len(y_train)\n",
    "    for minibatch_idx in tqdm(range(N // minibatch_size)):\n",
    "        optimizer.zero_grad()\n",
    "        loss = None\n",
    "        for idx in range(minibatch_size):\n",
    "            text = X_train[minibatch_idx * minibatch_size + idx]\n",
    "            text = torch.unsqueeze(text, 1)\n",
    "            labelIdx = y_train[minibatch_idx * minibatch_size + idx]\n",
    "            log_probs = model(text)\n",
    "            text_loss = model.compute_loss(log_probs.view(1,-1), torch.tensor([labelIdx]))\n",
    "            running_loss += text_loss\n",
    "            if loss is None:\n",
    "                loss = text_loss\n",
    "            else:\n",
    "                loss += text_loss\n",
    "            pred_label = torch.argmax(log_probs)\n",
    "            correct += int(pred_label == labelIdx)\n",
    "            total += 1\n",
    "        loss = loss / minibatch_size\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss.append(running_loss / N)\n",
    "    train_acc.append(correct / total)\n",
    "    print(\"Training completed for epoch: {}\".format(epoch + 1))\n",
    "    print(\"Time for train: {}\".format(time.time() - start_time))\n",
    "    print(\"Accuracy: {} Loss: {}\".format(correct / total, train_loss[-1]))\n",
    "    \n",
    "    #validation\n",
    "    running_loss = 0.0\n",
    "    model.eval()\n",
    "    optimizer.zero_grad()\n",
    "    print(\"Validation started for epoch: {}\".format(epoch + 1))\n",
    "    X_test, y_test = shuffle(X_test, y_test)\n",
    "    start_time = time.time()\n",
    "    correct = total = 0\n",
    "    predictions = []\n",
    "    N = len(y_test)\n",
    "    for minibatch_idx in tqdm(range(N // minibatch_size)):\n",
    "        optimizer.zero_grad()\n",
    "        for idx in range(minibatch_size):\n",
    "            text = X_test[minibatch_idx * minibatch_size + idx]\n",
    "            text = torch.unsqueeze(text, 1)\n",
    "            labelIdx = y_test[minibatch_idx * minibatch_size + idx]\n",
    "            log_probs = model(text)\n",
    "            text_loss = model.compute_loss(log_probs.view(1,-1), torch.tensor([labelIdx]))\n",
    "            running_loss += text_loss\n",
    "            pred_label = torch.argmax(log_probs)\n",
    "            correct += int(pred_label == labelIdx)\n",
    "            total += 1\n",
    "    curr_loss = running_loss / N\n",
    "    test_loss.append(curr_loss)\n",
    "    test_acc.append(correct / total)\n",
    "    print(\"Validation completed for epoch: {}\".format(epoch + 1))\n",
    "    print(\"Time for validation: {}\".format(time.time() - start_time))\n",
    "    print(\"Accuracy: {} Loss: {}\".format(correct / total, curr_loss))\n",
    "    if len(test_loss) > 3 and curr_loss >= test_loss[-3]:\n",
    "        print(\"Stopping progress: no longer learning for validation\")\n",
    "        break\n",
    "\n",
    "plt.plot(train_loss, label='train loss')\n",
    "plt.plot(test_loss, label='test loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_acc, label='train acc')\n",
    "plt.plot(test_acc, label='test acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
