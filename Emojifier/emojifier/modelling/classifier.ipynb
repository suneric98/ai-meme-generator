{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import emoji\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets\n",
      "['Yâ€™all canâ€™t get tender when I donâ€™t respond ğŸ˜‚\\n', 'dawg this game finna blow me ğŸ˜‚\\n', 'ğŸ˜‚ ğŸ˜‚ ğŸ˜‚\\n']\n",
      "Chosen emojis\n",
      "['ğŸ˜‚', 'ğŸ˜', 'ğŸ˜­', 'ğŸ˜Š', 'ğŸ’•', 'ğŸ˜’', 'ğŸ˜‰', 'ğŸ‘Œ', 'ğŸ‘', 'ğŸ™', 'ğŸ‘€', 'ğŸ”¥', 'ğŸ’¯', 'ğŸ‘', 'ğŸ’ª']\n"
     ]
    }
   ],
   "source": [
    "# getting the twitter comments\n",
    "DATA_PATH = '../data/twitter-data-cleaned.txt'\n",
    "with open(DATA_PATH, 'r',  encoding=\"utf-8\") as f:\n",
    "    data = f.readlines()\n",
    "print('Tweets')\n",
    "print(data[:3])\n",
    "\n",
    "# getting our chosen emojis\n",
    "SELECTED_EMOJIS_PATH = '../data/best-emojis.json'\n",
    "with open(SELECTED_EMOJIS_PATH, 'r') as f:\n",
    "    EMOJIS = json.load(f)\n",
    "EMOJI_CHARS = [e['char'] for e in EMOJIS]\n",
    "print('Chosen emojis')\n",
    "print(EMOJI_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ˜‚\n",
      "192\n",
      "[['i', 'donâ€™t', 'respond'], ['finna', 'blow', 'me'], ['we', 'in', 'trouble']]\n",
      "ğŸ˜\n",
      "153\n",
      "[['scramble', 'city', 'ksw55'], ['just', 'wow', 'amazing'], ['the', 'second', 'pic']]\n",
      "ğŸ˜­\n",
      "164\n",
      "[['miss', 'paris', 'tho'], ['theyâ€™re', 'so', 'cuteee'], ['theyâ€™re', 'so', 'cuteee']]\n",
      "ğŸ˜Š\n",
      "114\n",
      "[['hi', 'can', 'u'], ['all', 'of', 'them'], ['harming', 'other', 'people']]\n",
      "ğŸ’•\n",
      "133\n",
      "[['olivia', 'newton', 'john'], ['amp', 'baby', 'gia'], ['do', 'the', 'job']]\n",
      "ğŸ˜’\n",
      "98\n",
      "[['starters', 'on', 'defense'], ['talk', 'to', 'em'], ['know', 'the', 'feeling']]\n",
      "ğŸ˜‰\n",
      "109\n",
      "[['gets', 'literally', '25000+'], ['amp', 'gurlish', 'side'], ['youâ€™re', 'right']]\n",
      "ğŸ‘Œ\n",
      "110\n",
      "[['make', 'people', 'laugh'], ['thanks'], ['war', 'here', 'soon']]\n",
      "ğŸ‘\n",
      "123\n",
      "[['it', 'up', 'brother'], ['thanks', 'venting', 'to'], ['youâ€™re', 'very', 'welcome']]\n",
      "ğŸ™\n",
      "148\n",
      "[['the', 'shirt', 'please'], ['to', 'see', 'again'], ['my', 'dear', 'friend']]\n",
      "ğŸ‘€\n",
      "123\n",
      "[['when', 'life', 'gets'], ['rt', 'that', 'tweet'], ['one', 'is', 'from']]\n",
      "ğŸ”¥\n",
      "190\n",
      "[['install', 'golden', 'match'], ['this', 'gorgeous', 'look'], ['itâ€™s', 'lit', 'enjoy']]\n",
      "ğŸ’¯\n",
      "114\n",
      "[['ï¸'], ['art', 'action', 'drama'], ['hall', 'of', 'fame']]\n",
      "ğŸ‘\n",
      "207\n",
      "[['would', 'be', 'awesome'], ['hall', 'of', 'fame'], ['epic', 'classic']]\n",
      "ğŸ’ª\n",
      "124\n",
      "[['love', 'of', 'all'], ['love', 'of', 'all'], ['bathampbâ€¦']]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing the data\n",
    "from parsing import Tokenizer, TokenType, Token\n",
    "tokenizer = Tokenizer(EMOJI_CHARS)\n",
    "# take 3 previous words as context for the emoji\n",
    "context = {e:[] for e in EMOJI_CHARS}\n",
    "for tweet in data:\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    for i,token in enumerate(tokens):\n",
    "        if token.token_type == TokenType.EMOJIS:\n",
    "            closest = tokenizer.findClosest3Words(tokens, i)\n",
    "            if closest:\n",
    "                context[token.raw].append(closest)\n",
    "\n",
    "for e, words in context.items():\n",
    "    print(e)\n",
    "    print(len(words))\n",
    "    print(words[:3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
