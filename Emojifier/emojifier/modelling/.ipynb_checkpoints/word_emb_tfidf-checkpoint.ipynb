{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import emoji\n",
    "from os.path import join\n",
    "import json\n",
    "import time\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from gensim.models import FastText\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets\n",
      "['üí´ IT‚ÄôS BURGER MONDAY, CBUS! üí´ Order any Gourmet Burger with a draft beer üçîüç∫ ifor just $9.99 when you dine in. üí™ Cer‚Ä¶', 'Stay strong üí™ You are an amazing man. I appreciate the truth that is told.', 'Code Sale Awesome from Styli üí™']\n",
      "122179\n",
      "Chosen emojis\n",
      "['üòÇ', 'üòç', 'üò≠', 'üòä', 'üíï', 'üòí', 'üòâ', 'üëå', 'üëç', 'üôè', 'üëÄ', 'üî•', 'üíØ', 'üëè', 'üí™']\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# getting the twitter comments\n",
    "DATA_PATH = join('..','data','twitter-data-cleaned.txt')\n",
    "with open(DATA_PATH, 'r',  encoding=\"utf-8\") as f:\n",
    "    data = f.readlines()\n",
    "data = [d.strip() for d in data if d.strip() != '']\n",
    "print('Tweets')\n",
    "print(data[:3])\n",
    "print(len(data))\n",
    "\n",
    "# getting our chosen emojis\n",
    "SELECTED_EMOJIS_PATH = join('..','data','best-emojis.json')\n",
    "with open(SELECTED_EMOJIS_PATH, 'r', encoding='utf-8') as f:\n",
    "    EMOJIS = json.load(f)\n",
    "EMOJI_CHARS = [e['char'] for e in EMOJIS]\n",
    "print('Chosen emojis')\n",
    "print(EMOJI_CHARS)\n",
    "print(len(EMOJI_CHARS))\n",
    "\n",
    "ALL_EMOJIS = set(emoji.emojize(emoji_code) for emoji_code in emoji.UNICODE_EMOJI.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üòÇ\n",
      "8458\n",
      "[['familiar', 'with', 'my', 'face', 'too'], ['bollywood', 'reality'], ['though', 'your', 'debates', 'be', 'blasphemy']]\n",
      "üòç\n",
      "7330\n",
      "[['figura', 'is', 'a', 'start', 'today'], ['200', 'days', 'to', 'go'], ['200', 'days', 'to', 'go']]\n",
      "üò≠\n",
      "7464\n",
      "[['lol', 'procrastination', 'almost', 'had', 'me'], ['just', 'change', 'my', 'first', 'diaper'], ['52', 'yuan', 'for', 'this', 'set']]\n",
      "üòä\n",
      "6352\n",
      "[['you', 'today', 'credit', 'stacie', 'swift'], ['fantastic', 'well', 'done', 'ladies'], ['congratulations', 'fam', 'all', 'the', 'best']]\n",
      "üíï\n",
      "6485\n",
      "[['happy', 'birthday', 'my', 'mochiiii', 'saranghae'], ['got', 'this', 'soon', 'youre', 'done'], ['we', 'got', 'this']]\n",
      "üòí\n",
      "5873\n",
      "[['Ô∏è', 'dunk', 'it', 'ang', 'hina'], ['comes', '17', 'on', 'uber', 'eats'], ['comes', '17', 'on', 'uber', 'eats']]\n",
      "üòâ\n",
      "5932\n",
      "[['start', 'today', 'choose', 'your', 'package'], ['the', 'richest', 'man', 'your', 'age'], ['yall', 'know', 'the', 'rest']]\n",
      "üëå\n",
      "6035\n",
      "[['yess'], ['nice', 'one', 'you', 'are', 'a'], ['the', 'best', 'and', 'great', 'artist']]\n",
      "üëç\n",
      "6463\n",
      "[['promises', 'made'], ['88m', 'tweets', 'on', 'sarkaruvaaripaata'], ['you', 'succeed', 'we', 'all', 'succeed']]\n",
      "üôè\n",
      "6741\n",
      "[['keep', 'going', 'more', 'power', 'to'], ['facts', 'i‚Äôm', 'very', 'interested', 'and'], ['the', 'thread', 'and', 'tweet', 'well']]\n",
      "üëÄ\n",
      "5920\n",
      "[['we', 'you', 'post', 'this', 'on'], ['live', 'tonight', 'at', '7pm', 'mst'], ['make', 'it', '5', 'k', 'tweets']]\n",
      "üî•\n",
      "9914\n",
      "[['wickets', 'broken'], ['nnz'], ['nnz']]\n",
      "üíØ\n",
      "6364\n",
      "[['it', 'up', 'bro', 'full', 'support'], ['age', 'i', 'didn‚Äôt', 'say', 'that'], ['200', 'days', 'to', 'go']]\n",
      "üëè\n",
      "9216\n",
      "[['future', 'this', 'young', 'mans', 'got'], ['„ÄÇ', '„ÄÇ', '„ÄÇ'], ['trump', '2020']]\n",
      "üí™\n",
      "6479\n",
      "[['999', 'when', 'you', 'dine', 'in'], ['stay', 'strong', 'you', 'are', 'an'], ['code', 'sale', 'awesome', 'from', 'styli']]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing the data\n",
    "from parsing import Tokenizer, TokenType, Token\n",
    "tokenizer = Tokenizer(EMOJI_CHARS)\n",
    "# take 3 previous words as context for the emoji\n",
    "context = {e:[] for e in EMOJI_CHARS}\n",
    "emojiToId = {e:i for i,e in enumerate(EMOJI_CHARS)}\n",
    "\n",
    "for tweet in data:\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    for i,token in enumerate(tokens):\n",
    "        if token.token_type == TokenType.EMOJIS:\n",
    "            closest = tokenizer.findClosestNWords(5, tokens, i)\n",
    "            if closest:\n",
    "                context[token.raw].append(closest)\n",
    "\n",
    "for e, words in context.items():\n",
    "    print(e)\n",
    "    print(len(words))\n",
    "    print(words[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.17454654,  0.40511113, -0.09391642,  0.72003835, -0.07052381,\n",
       "       -0.08196047, -0.31681183,  0.02695159, -0.7591647 , -0.13951467,\n",
       "       -0.3060772 ,  0.14738002,  0.39180475, -0.05031242, -0.33401808,\n",
       "       -0.20664431,  0.57543015,  0.0742536 ,  0.48540726,  0.43160683,\n",
       "       -0.57935655, -0.40128928,  0.08931972, -0.41961792,  0.24711417,\n",
       "        0.27860305,  0.5956422 ,  0.39462286, -1.0656201 ,  0.4321106 ,\n",
       "       -0.2288525 ,  0.5105419 , -0.41794413, -0.11967197,  0.4788728 ,\n",
       "       -0.3365009 , -0.03886222, -1.2256435 , -0.4413131 ,  0.33565876,\n",
       "       -0.2976866 ,  0.04310591, -0.10757468, -0.34711736, -0.05832572,\n",
       "       -0.9738132 , -0.4167317 ,  0.7088729 ,  0.1166968 ,  0.14113304,\n",
       "        0.12601767,  0.3617158 , -0.01839591, -0.30152157,  0.24930586,\n",
       "       -0.7027018 , -0.13906552, -0.13156067,  0.79356027,  0.13548094,\n",
       "       -0.04003473,  0.3007476 , -0.7594328 , -0.39455166, -0.17626712,\n",
       "       -0.3215084 , -0.11813012,  0.14159289,  0.1951103 , -0.20376056,\n",
       "       -0.14102818,  1.0088058 ,  0.93528366, -0.4578043 ,  0.28075236,\n",
       "        0.09952775,  0.04204928, -0.33357   ,  0.5630153 ,  1.0513438 ,\n",
       "        0.11687878,  0.03080976, -0.81986445, -0.04234402, -0.8498685 ,\n",
       "       -0.09708723,  0.23493502,  0.07875046,  0.25424075,  0.16724926,\n",
       "       -0.39766544, -0.0946226 ,  0.45422935, -0.7061704 , -0.35105285,\n",
       "       -0.20736474, -0.54161966, -0.36676058, -0.31426662,  0.22677724],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making X and y for TFIDF as a baseline idea of how good our accuracy can expect to be\n",
    "X_words = []\n",
    "X_tokens = []\n",
    "y = []\n",
    "\n",
    "sentences = set()\n",
    "for e, words in context.items():\n",
    "    for i,word_list in enumerate(words):\n",
    "        sentence = ' '.join(word_list)\n",
    "        if sentence in sentences:\n",
    "            continue\n",
    "        sentences.add(sentence)\n",
    "        X_tokens.append(word_list)\n",
    "        X_words.append(sentence)\n",
    "        y.append(emojiToId[e])\n",
    "print(len(y))\n",
    "\n",
    "model = FastText(X_tokens, size=100, window=3, min_count=1, iter=10, sorted_vocab=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.17454654  0.40511113 -0.09391642  0.72003835 -0.07052381 -0.08196047\n",
      " -0.31681183  0.02695159 -0.7591647  -0.13951467 -0.3060772   0.14738002\n",
      "  0.39180475 -0.05031242 -0.33401808 -0.20664431  0.57543015  0.0742536\n",
      "  0.48540726  0.43160683 -0.57935655 -0.40128928  0.08931972 -0.41961792\n",
      "  0.24711417  0.27860305  0.5956422   0.39462286 -1.0656201   0.4321106\n",
      " -0.2288525   0.5105419  -0.41794413 -0.11967197  0.4788728  -0.3365009\n",
      " -0.03886222 -1.2256435  -0.4413131   0.33565876 -0.2976866   0.04310591\n",
      " -0.10757468 -0.34711736 -0.05832572 -0.9738132  -0.4167317   0.7088729\n",
      "  0.1166968   0.14113304  0.12601767  0.3617158  -0.01839591 -0.30152157\n",
      "  0.24930586 -0.7027018  -0.13906552 -0.13156067  0.79356027  0.13548094\n",
      " -0.04003473  0.3007476  -0.7594328  -0.39455166 -0.17626712 -0.3215084\n",
      " -0.11813012  0.14159289  0.1951103  -0.20376056 -0.14102818  1.0088058\n",
      "  0.93528366 -0.4578043   0.28075236  0.09952775  0.04204928 -0.33357\n",
      "  0.5630153   1.0513438   0.11687878  0.03080976 -0.81986445 -0.04234402\n",
      " -0.8498685  -0.09708723  0.23493502  0.07875046  0.25424075  0.16724926\n",
      " -0.39766544 -0.0946226   0.45422935 -0.7061704  -0.35105285 -0.20736474\n",
      " -0.54161966 -0.36676058 -0.31426662  0.22677724]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# checking fasttext embeddings\n",
    "print(model['hello'])\n",
    "print(len(model['hello']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: 0\n",
      "Empty sent: 0\n"
     ]
    }
   ],
   "source": [
    "# creating TFIDF sentence embeddings\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(X_words)\n",
    "tfidf_feat = vectorizer.get_feature_names()\n",
    "\n",
    "sent_emb = []\n",
    "errors = []\n",
    "empty_sent = 0\n",
    "for i, tweet in enumerate(X_tokens):\n",
    "    sent_vec = np.zeros(100)\n",
    "    weight_sum = 0\n",
    "    for word in tweet:\n",
    "        try:\n",
    "            emb = model.wv[word]\n",
    "#             tfidf_val = tfidf[i, tfidf_feat.index(word)]\n",
    "            tfidf_val = 1\n",
    "            sent_vec += (emb * tfidf_val)\n",
    "            weight_sum += 1\n",
    "        except:\n",
    "            errors.append(word)\n",
    "            pass\n",
    "    if weight_sum == 0:\n",
    "        empty_sent += 1\n",
    "    weight_sum = max(1, weight_sum)\n",
    "    sent_vec /= weight_sum\n",
    "    sent_emb.append(sent_vec)\n",
    "print('Errors: {}'.format(len(errors)))\n",
    "print('Empty sent: {}'.format(empty_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: rf\n",
      "Accuracy for: 0.15 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "# training models using sentence embeddings\n",
    "\n",
    "clfs = {'rf': RandomForestClassifier()}\n",
    "for name, clf in clfs.items():\n",
    "    scores = cross_val_score(clf, sent_emb, y, cv=5)\n",
    "    print(\"Model: \" + name)\n",
    "    print(\"Accuracy for: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
