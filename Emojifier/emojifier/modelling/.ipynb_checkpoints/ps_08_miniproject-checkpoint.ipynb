{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem set 8: Mini-project\n",
    "\n",
    "We've put some effort into building our collection (see problem set 7 for details and for links to texts and to metadata). Now it's time to learn something about it. You already have lots of excellent ideas for how to apply the tools we've learned about so far. It's also a good time of the semester to review what we have learned and practice applying it in less structured settings.\n",
    "\n",
    "**You will work by yourself or in a group of up to three people** to complete a short project applying methods from the previous weeks to this collection. You will turn in the completed project as a single notebook (one submission per group) with the following sections:\n",
    "\n",
    "1. **Question(s).** Describe what you wanted to learn. Suggest several possible answers or hypotheses, and describe in general terms what you might expect to see if each of these answers were true (save specific measurements for the next section). For example, many students want to know the difference between horror and non-horror, or between detective stories and horror fiction, but there are many ways to operationalize this question. You do not need to limit yourself to questions of genre. **Note that your question should be interesting! If the answer is obvious before you begin, or if it's something the importance of which you cannot explain, your grade will suffer (a lot).** (10 points)\n",
    "\n",
    "1. **Methods.** Describe how you will use computational methods presented so far in this class to answer your question. What do the computational tools do, and how does their output relate to your question? Describe how you will process the collection into a form suitable for a model or algorithm and why you have processed it the way you have. (10 points)\n",
    "\n",
    "1. **Code.** Carry out your experiments. Code should be correct (no errors) and focused (unneeded code from examples is removed). Use the notebook format effectively: code may be incorporated into multiple sections. (20 points)\n",
    "\n",
    "1. **Results and discussion.** Use sorted lists, tables, and visual presentations to make your argument. Excellent projects will provide multiple views of results, and follow up on any apparent outliers or strange cases, including through careful reading of the original documents. (40 points)\n",
    "\n",
    "1. **Reflection.** Describe your experience in this process. What was harder or easier than you expected? What compromises or negotiations did you have to accept to match the collection, the question, and the methods? What would you try next? (10 points)\n",
    "\n",
    "1. **Responsibility and resources consulted.** Credit any online sources (Stack Overflow, blog posts, documentation) that you found helpful. (0 points, but -10 if missing)\n",
    "    * **If you worked in a group**, set up a group submission in CMS. Each group member should submit (via CMS) a separate text file in which they describe each member's (including their own) contributions to the project.\n",
    "    * Most people will turn in *either* a completed notebook for their solo project *or* a responsibility statement. The only people who will submit both files are those who are the designated submitter for their group. Don't worry if CMS warns you about a missing file (unless you're the group submitter).\n",
    "\n",
    "Note that 10 points will be carried over from problem set 7.\n",
    "\n",
    "**We will grade this work based on accuracy, thoroughness, creativity, reflectiveness, and quality of presentation.**\n",
    "\n",
    "**Scope:** this is a *mini*-project, with a short deadline. We are expecting work that is consistent with that timeframe, but that is serious, thoughtful, and rigorous. This problem set will almost certainly require more time and effort than many of the others. **For group work, the expected scope grows linearly with the number of participants.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Project team\n",
    "\n",
    "List here the members of your project team, including yourself.\n",
    "\n",
    "Eric Sun, Shalin Mehta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Question(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "### Is there a significant difference in the way male and female writers create their books, and can we classify this?\n",
    "\n",
    "Our first question is basically to predict whether a book is written by a male or female author. We believe that this is accomplishable because from the papers we've been reading throughout this semester, it seems like previous work suggests that there are ways to differentiate male and female writers (for example, in Piper's \"Characterization\" they talk about how they found more female authors in older books wrote more introspective characters). \n",
    "\n",
    "Therefore we plan to use a bag of words model (Tfidf) to predict whether a text is written by a male or female author. We believe this will perform reasonably well, because we think that some words might be used a lot more by female authors more so than males. We also think this is a good dataset to try this on because it's very balanced in terms of gender. One thing we think might affect this is the genre of the books we use. For example, detective fiction novels might be very similar regardless of the author's gender, because those typically focus on very specific things.\n",
    "\n",
    "+ Hypothesis: we think that the author's gender can be predicted through the words they use in their text\n",
    "+ Our suggested answers: we might expect to see that female authors tend to use more introspective words as suggested in Piper's paper. Or we might find that it's very difficult to differentiate the two, which might be a result of the genres this corpus is made of.\n",
    "\n",
    "## Question 2\n",
    "\n",
    "### Can we predict whether a novel was adapted or not based on multiple features?\n",
    "\n",
    "We think we can predict pretty well whether a novel was adapted or not based on features like (# of downloads, year it was published, wordcount, pov). We believe these features can in a way measure how 'popular' or 'well-received' a text was, and therefore whether or not it was adapted. We intend to use something like logistic regression or random forest to predict this.\n",
    "\n",
    "+ Hypothesis: we believe that whether or not a novel is adapted can be predicted using various features like # of downloads and year it was published.\n",
    "+ Our suggested answers: we think that these features will be able to predict pretty accurately, but if it isn't then it might be due to things like project gutemburg's download count not being a good enough indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "For this we intend to use Tfidf vectorizer, which is basically a normalized bag of words model using the inverse document frequency as a weight. We chose TFIDF over just a normal BoW counter because it's normalized and provides more unique words. We intend to remove stopwords using the sklearn stopwords from TFIDF. We believe this is okay because we're not super familiar with our dataset and don't have specific words we are certain we want to keep, so these stopwords are good enough to help remove too common words that aren't helpful.\n",
    "\n",
    "We intend to try a couple of different models. Our first model is Random Forest, because it's a very strong method that's very easy to use and will hopefully perform very well. Our second model is logistic regression, which is also very powerful. Both methods can give us a percentage value of how correct our value is too (random forest by telling us the percentage of trees that were correct, and logistic regression which just gives us the percent value). We also intend to try SVM, which is a very powerful classifier that can give extremely good results.\n",
    "\n",
    "## Question 2\n",
    "\n",
    "For this question we intend to simply use the features from the csv metadata file. The features we intend to use are  \n",
    "\n",
    "number of downloads, wordcount, year of publish, point of view, and genre (horror, detective, or neither horror nor detective).\n",
    "\n",
    "We believe these features are helpful in predicting the outcome, and will measure their effectiveness through our models.\n",
    "\n",
    "The models we intend to use are random forest and logistic regression. These are good models because they can tell us how good our variables were in predicting the outcome, and can also give us a percentage for how confident they are in an answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports (all of them!)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from   sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>owner</th>\n",
       "      <th>check_1</th>\n",
       "      <th>check_2</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>author_surname</th>\n",
       "      <th>author_givenname</th>\n",
       "      <th>year</th>\n",
       "      <th>genre</th>\n",
       "      <th>form</th>\n",
       "      <th>country</th>\n",
       "      <th>wordcount</th>\n",
       "      <th>language</th>\n",
       "      <th>pov</th>\n",
       "      <th>gender</th>\n",
       "      <th>horror</th>\n",
       "      <th>detective</th>\n",
       "      <th>adaptation</th>\n",
       "      <th>downloads</th>\n",
       "      <th>source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>138</td>\n",
       "      <td>123</td>\n",
       "      <td>92</td>\n",
       "      <td>138</td>\n",
       "      <td>138</td>\n",
       "      <td>138</td>\n",
       "      <td>138</td>\n",
       "      <td>137</td>\n",
       "      <td>138</td>\n",
       "      <td>138</td>\n",
       "      <td>138</td>\n",
       "      <td>135</td>\n",
       "      <td>138</td>\n",
       "      <td>138</td>\n",
       "      <td>138</td>\n",
       "      <td>138</td>\n",
       "      <td>138</td>\n",
       "      <td>136</td>\n",
       "      <td>132</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>69</td>\n",
       "      <td>53</td>\n",
       "      <td>50</td>\n",
       "      <td>138</td>\n",
       "      <td>137</td>\n",
       "      <td>81</td>\n",
       "      <td>84</td>\n",
       "      <td>83</td>\n",
       "      <td>84</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>134</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>as2778</td>\n",
       "      <td>db758</td>\n",
       "      <td>swt53</td>\n",
       "      <td>The_Great_God_Pan.txt</td>\n",
       "      <td>Plague Ship</td>\n",
       "      <td>Norton</td>\n",
       "      <td>Andre</td>\n",
       "      <td>1922</td>\n",
       "      <td>Science Fiction</td>\n",
       "      <td>novel</td>\n",
       "      <td>us</td>\n",
       "      <td>60020</td>\n",
       "      <td>en</td>\n",
       "      <td>third</td>\n",
       "      <td>female</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>262</td>\n",
       "      <td>http://www.gutenberg.org/ebooks/4047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>118</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>134</td>\n",
       "      <td>75</td>\n",
       "      <td>71</td>\n",
       "      <td>97</td>\n",
       "      <td>104</td>\n",
       "      <td>82</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         owner check_1 check_2               filename        title  \\\n",
       "count      138     123      92                    138          138   \n",
       "unique      69      53      50                    138          137   \n",
       "top     as2778   db758   swt53  The_Great_God_Pan.txt  Plague Ship   \n",
       "freq         3       4       4                      1            2   \n",
       "\n",
       "       author_surname author_givenname  year            genre   form country  \\\n",
       "count             138              138   137              138    138     138   \n",
       "unique             81               84    83               84      6      11   \n",
       "top            Norton            Andre  1922  Science Fiction  novel      us   \n",
       "freq               10               10     5                8    118      64   \n",
       "\n",
       "       wordcount language    pov  gender horror detective adaptation  \\\n",
       "count        135      138    138     138    138       138        136   \n",
       "unique       134        3      2       2      2         2          2   \n",
       "top        60020       en  third  female  False     False      False   \n",
       "freq           2      134     75      71     97       104         82   \n",
       "\n",
       "       downloads                            source_url  \n",
       "count        132                                   137  \n",
       "unique       129                                   136  \n",
       "top          262  http://www.gutenberg.org/ebooks/4047  \n",
       "freq           2                                     2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# getting our csv metadata and files\n",
    "METADATA_FILE = 'info3350_lit_corpus.csv'\n",
    "DATA_FOLDER = 'corpus'\n",
    "\n",
    "data = pd.read_csv(METADATA_FILE)\n",
    "display(data.describe())\n",
    "\n",
    "# getting the file paths\n",
    "filenames = data['filename']\n",
    "# filenames = [f.strip() for f in list(filenames)]\n",
    "filepaths = [os.path.join('corpus',f) for f in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a check to find out if any of the files aren't accessible / causing issues\n",
    "for f in filepaths:\n",
    "    if not os.path.isfile(f):\n",
    "        print(f)\n",
    "\n",
    "for f in filepaths:\n",
    "    try:\n",
    "        file = open(f, encoding='utf-8')\n",
    "        text = file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from Prof's code from HW: makes a colored dictionary of results\n",
    "def compare_scores(scores_dict):\n",
    "    '''\n",
    "    Takes a dictionary of cross_validate scores.\n",
    "    Returns a color-coded Pandas dataframe that summarizes those scores.\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(scores_dict).T.applymap(np.mean).style.background_gradient(cmap='RdYlGn')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Question 1\n",
    "\n",
    "# Data Gathering\n",
    "vectorizer = TfidfVectorizer(\n",
    "    input='filename',\n",
    "    encoding='utf-8',\n",
    "    binary=False,\n",
    "    norm='l2',\n",
    "#     max_df=0.8,\n",
    "    min_df=0.5,\n",
    "    stop_words='english',\n",
    "    use_idf=True\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(filepaths)\n",
    "y = [0 if g == 'female' else 1 for g in list(data['gender'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_2ca07638_162c_11eb_91be_82185fe7d140row0_col0,#T_2ca07638_162c_11eb_91be_82185fe7d140row0_col1,#T_2ca07638_162c_11eb_91be_82185fe7d140row1_col3,#T_2ca07638_162c_11eb_91be_82185fe7d140row5_col2,#T_2ca07638_162c_11eb_91be_82185fe7d140row5_col4,#T_2ca07638_162c_11eb_91be_82185fe7d140row5_col5{\n",
       "            background-color:  #006837;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_2ca07638_162c_11eb_91be_82185fe7d140row0_col2,#T_2ca07638_162c_11eb_91be_82185fe7d140row0_col5{\n",
       "            background-color:  #f57748;\n",
       "            color:  #000000;\n",
       "        }#T_2ca07638_162c_11eb_91be_82185fe7d140row0_col3,#T_2ca07638_162c_11eb_91be_82185fe7d140row3_col2,#T_2ca07638_162c_11eb_91be_82185fe7d140row3_col4,#T_2ca07638_162c_11eb_91be_82185fe7d140row3_col5,#T_2ca07638_162c_11eb_91be_82185fe7d140row5_col0,#T_2ca07638_162c_11eb_91be_82185fe7d140row5_col1{\n",
       "            background-color:  #a50026;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_2ca07638_162c_11eb_91be_82185fe7d140row0_col4{\n",
       "            background-color:  #eb5a3a;\n",
       "            color:  #000000;\n",
       "        }#T_2ca07638_162c_11eb_91be_82185fe7d140row1_col0,#T_2ca07638_162c_11eb_91be_82185fe7d140row4_col0{\n",
       "            background-color:  #db382b;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_2ca07638_162c_11eb_91be_82185fe7d140row1_col1{\n",
       "            background-color:  #fee28f;\n",
       "            color:  #000000;\n",
       "        }#T_2ca07638_162c_11eb_91be_82185fe7d140row1_col2,#T_2ca07638_162c_11eb_91be_82185fe7d140row1_col5{\n",
       "            background-color:  #8ccd67;\n",
       "            color:  #000000;\n",
       "        }#T_2ca07638_162c_11eb_91be_82185fe7d140row1_col4{\n",
       "            background-color:  #c9e881;\n",
       "            color:  #000000;\n",
       "        }#T_2ca07638_162c_11eb_91be_82185fe7d140row2_col0{\n",
       "            background-color:  #de402e;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_2ca07638_162c_11eb_91be_82185fe7d140row2_col1{\n",
       "            background-color:  #a7d96b;\n",
       "            color:  #000000;\n",
       "        }#T_2ca07638_162c_11eb_91be_82185fe7d140row2_col2,#T_2ca07638_162c_11eb_91be_82185fe7d140row2_col5{\n",
       "            background-color:  #f88950;\n",
       "            color:  #000000;\n",
       "        }#T_2ca07638_162c_11eb_91be_82185fe7d140row2_col3{\n",
       "            background-color:  #feefa3;\n",
       "            color:  #000000;\n",
       "        }#T_2ca07638_162c_11eb_91be_82185fe7d140row2_col4{\n",
       "            background-color:  #fdb365;\n",
       "            color:  #000000;\n",
       "        }#T_2ca07638_162c_11eb_91be_82185fe7d140row3_col0{\n",
       "            background-color:  #f16640;\n",
       "            color:  #000000;\n",
       "        }#T_2ca07638_162c_11eb_91be_82185fe7d140row3_col1{\n",
       "            background-color:  #5db961;\n",
       "            color:  #000000;\n",
       "        }#T_2ca07638_162c_11eb_91be_82185fe7d140row3_col3{\n",
       "            background-color:  #f36b42;\n",
       "            color:  #000000;\n",
       "        }#T_2ca07638_162c_11eb_91be_82185fe7d140row4_col1{\n",
       "            background-color:  #feea9b;\n",
       "            color:  #000000;\n",
       "        }#T_2ca07638_162c_11eb_91be_82185fe7d140row4_col2,#T_2ca07638_162c_11eb_91be_82185fe7d140row4_col4,#T_2ca07638_162c_11eb_91be_82185fe7d140row4_col5{\n",
       "            background-color:  #93d168;\n",
       "            color:  #000000;\n",
       "        }#T_2ca07638_162c_11eb_91be_82185fe7d140row4_col3{\n",
       "            background-color:  #0e8245;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_2ca07638_162c_11eb_91be_82185fe7d140row5_col3{\n",
       "            background-color:  #fff3ac;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_2ca07638_162c_11eb_91be_82185fe7d140\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >fit_time</th>        <th class=\"col_heading level0 col1\" >score_time</th>        <th class=\"col_heading level0 col2\" >test_accuracy</th>        <th class=\"col_heading level0 col3\" >test_f1</th>        <th class=\"col_heading level0 col4\" >test_f1_macro</th>        <th class=\"col_heading level0 col5\" >test_f1_micro</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_2ca07638_162c_11eb_91be_82185fe7d140level0_row0\" class=\"row_heading level0 row0\" >random forest</th>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row0_col0\" class=\"data row0 col0\" >1.972171</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row0_col1\" class=\"data row0 col1\" >0.142947</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row0_col2\" class=\"data row0 col2\" >0.730952</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row0_col3\" class=\"data row0 col3\" >0.724658</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row0_col4\" class=\"data row0 col4\" >0.725425</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row0_col5\" class=\"data row0 col5\" >0.730952</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2ca07638_162c_11eb_91be_82185fe7d140level0_row1\" class=\"row_heading level0 row1\" >svm-linear</th>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row1_col0\" class=\"data row1 col0\" >0.259116</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row1_col1\" class=\"data row1 col1\" >0.060757</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row1_col2\" class=\"data row1 col2\" >0.746296</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row1_col3\" class=\"data row1 col3\" >0.762020</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row1_col4\" class=\"data row1 col4\" >0.740326</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row1_col5\" class=\"data row1 col5\" >0.746296</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2ca07638_162c_11eb_91be_82185fe7d140level0_row2\" class=\"row_heading level0 row2\" >svm-rbf</th>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row2_col0\" class=\"data row2 col0\" >0.283741</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row2_col1\" class=\"data row2 col1\" >0.100956</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row2_col2\" class=\"data row2 col2\" >0.731746</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row2_col3\" class=\"data row2 col3\" >0.741354</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row2_col4\" class=\"data row2 col4\" >0.729975</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row2_col5\" class=\"data row2 col5\" >0.731746</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2ca07638_162c_11eb_91be_82185fe7d140level0_row3\" class=\"row_heading level0 row3\" >svm-linear-c=0.8</th>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row3_col0\" class=\"data row3 col0\" >0.408696</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row3_col1\" class=\"data row3 col1\" >0.116782</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row3_col2\" class=\"data row3 col2\" >0.724603</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row3_col3\" class=\"data row3 col3\" >0.732070</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row3_col4\" class=\"data row3 col4\" >0.720017</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row3_col5\" class=\"data row3 col5\" >0.724603</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2ca07638_162c_11eb_91be_82185fe7d140level0_row4\" class=\"row_heading level0 row4\" >svm-rbf-c=0.8</th>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row4_col0\" class=\"data row4 col0\" >0.261718</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row4_col1\" class=\"data row4 col1\" >0.064137</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row4_col2\" class=\"data row4 col2\" >0.746032</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row4_col3\" class=\"data row4 col3\" >0.759926</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row4_col4\" class=\"data row4 col4\" >0.743448</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row4_col5\" class=\"data row4 col5\" >0.746032</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2ca07638_162c_11eb_91be_82185fe7d140level0_row5\" class=\"row_heading level0 row5\" >logistic</th>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row5_col0\" class=\"data row5 col0\" >0.040122</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row5_col1\" class=\"data row5 col1\" >0.004156</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row5_col2\" class=\"data row5 col2\" >0.753968</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row5_col3\" class=\"data row5 col3\" >0.741897</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row5_col4\" class=\"data row5 col4\" >0.752117</td>\n",
       "                        <td id=\"T_2ca07638_162c_11eb_91be_82185fe7d140row5_col5\" class=\"data row5 col5\" >0.753968</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f8b276db828>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model training\n",
    "models = {\n",
    "    'random forest':RandomForestClassifier(n_estimators=300),\n",
    "    'svm-linear':svm.SVC(kernel='linear'),\n",
    "    'svm-rbf':svm.SVC(kernel='rbf'),\n",
    "    'svm-linear-c=0.8':svm.SVC(kernel='linear', C=0.8),\n",
    "    'svm-rbf-c=0.8':svm.SVC(kernel='rbf', C=0.8),\n",
    "    'logistic':LogisticRegression()\n",
    "}\n",
    "\n",
    "scores = {}\n",
    "for name, model in models.items():\n",
    "    score = cross_validate(model, X, y, cv=5, \n",
    "                                  scoring=['accuracy', 'f1', 'f1_macro', 'f1_micro'], return_estimator=True)\n",
    "    models[name] = score['estimator'][np.argmax(score['test_accuracy'])] # save the best model!\n",
    "    del score['estimator'] # delete it so compare_scores still works\n",
    "    scores[name] = score\n",
    "compare_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantity 0.006698449897931407\n",
      "eager 0.005767653500411427\n",
      "impatiently 0.004901586915914168\n",
      "questioned 0.004442634777894013\n",
      "longed 0.004335125668674868\n",
      "horrible 0.0041779177705078985\n",
      "smoking 0.004085729332535447\n",
      "presently 0.004061796292280788\n",
      "devil 0.003771350012669546\n",
      "fright 0.003731517017168895\n",
      "thirty 0.0036316524084214485\n",
      "nest 0.003604400839163455\n"
     ]
    }
   ],
   "source": [
    "# Feature importance: find which words were most important in predicting outcome\n",
    "importance = models['random forest'].feature_importances_\n",
    "indices = np.argsort(importance)[::-1]\n",
    "words = vectorizer.get_feature_names()\n",
    "topWords = []\n",
    "for i,ind in enumerate(indices):\n",
    "    print(words[ind], importance[ind])\n",
    "    topWords.append(words[ind])\n",
    "    if i > 10: #get the best 10 words\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantity: 0.67 male 0.33 female\n",
      "eager: 0.36 male 0.64 female\n",
      "impatiently: 0.40 male 0.60 female\n",
      "questioned: 0.43 male 0.57 female\n",
      "longed: 0.21 male 0.79 female\n",
      "horrible: 0.71 male 0.29 female\n",
      "smoking: 0.71 male 0.29 female\n",
      "presently: 0.72 male 0.28 female\n",
      "devil: 0.66 male 0.34 female\n",
      "fright: 0.65 male 0.35 female\n",
      "thirty: 0.60 male 0.40 female\n",
      "nest: 0.35 male 0.65 female\n"
     ]
    }
   ],
   "source": [
    "# collecting the percentage of female/male authored books each word shows up in\n",
    "\n",
    "# count the # of times each of our top words appears in each book\n",
    "counter = CountVectorizer(\n",
    "    input='filename',\n",
    "    encoding='utf-8',\n",
    "    binary=False,\n",
    "    vocabulary=topWords\n",
    ")\n",
    "\n",
    "# count the number of times that word appears for male books or female books\n",
    "count_vector = counter.fit_transform(filepaths)\n",
    "wordToBook = {word:{'male':0,'female':0} for word in topWords}\n",
    "for i,file in enumerate(filepaths):\n",
    "    for j,word in enumerate(topWords):\n",
    "        if count_vector[i,j] > 0:\n",
    "            if y[i]: # male\n",
    "                wordToBook[word]['male'] += count_vector[i,j]\n",
    "            else:\n",
    "                wordToBook[word]['female'] += count_vector[i,j]\n",
    "\n",
    "# print our result as a percentage\n",
    "for word,value in wordToBook.items():\n",
    "    total = sum(list(value.values()))\n",
    "    female = 0\n",
    "    male = 0\n",
    "    for gender,count in value.items():\n",
    "        if gender == 'male':\n",
    "            male = count / total\n",
    "        else:\n",
    "            female = count / total\n",
    "    print(\"{}: {:.2f} male {:.2f} female\".format(word, male, female))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row\n",
      "And Then There Were None 1939 53,921 nan True\n",
      "Skipping row\n",
      "The Count of Monte Cristo  nan 464, 234 11093 True\n",
      "Skipping row\n",
      "The Great Gatsby 1925 48410 nan True\n",
      "Skipping row\n",
      "The Paradise Mystery 1920 nan 161 False\n",
      "Skipping row\n",
      "The Shadow Over Innsmouth 1936 nan nan True\n",
      "Skipping row\n",
      "The Sorcery Club 1912 nan 66 False\n",
      "Skipping row\n",
      "The Works of Edgar Allan Poe - Volume 4 by Edgar Allan Poe 1809-1849 85,975 719 False\n",
      "Skipping row\n",
      "The Works of Edgar Allan Poe - Volume 5 by Edgar Allan Poe 1809-1849 72179 924 True\n",
      "Skipping row\n",
      "The Age of Innocence 1920 101254 nan nan\n",
      "Skipping row\n",
      "Their Eyes Were Watching God 1937 75952 nan True\n",
      "Skipping row\n",
      "The Mist 1980 61568 nan True\n"
     ]
    }
   ],
   "source": [
    "# getting data and skipping values that are NAN or a range (ex. 1809-1849)\n",
    "\n",
    "# we get a list of each feature we want. If the value is missing or invalid, we just skip that entire row\n",
    "years = []\n",
    "wordcount = []\n",
    "downloads = []\n",
    "detective = []\n",
    "horror = []\n",
    "adaptation_y = []\n",
    "for _, row in data.iterrows():\n",
    "    try:\n",
    "        year = int(row['year'])\n",
    "        wc = int(row['wordcount'].replace(',',''))\n",
    "        dl = int(row['downloads'].replace(',',''))\n",
    "        years.append(year)\n",
    "        wordcount.append(wc)\n",
    "        downloads.append(dl)\n",
    "        det = 1 if row['detective'] else 0\n",
    "        detective.append(det)\n",
    "        hor = 1 if row['horror'] else 0\n",
    "        horror.append(hor)\n",
    "        adaptation = 1 if row['adaptation'] else 0\n",
    "        adaptation_y.append(adaptation)\n",
    "    except (ValueError, AttributeError): #skip the row because one of our features is invalid - nan or not a int\n",
    "        print(\"Skipping row\")\n",
    "        print(row['title'], row['year'], row['wordcount'], row['downloads'], row['adaptation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127, 5)\n",
      "127\n"
     ]
    }
   ],
   "source": [
    "# preprocessing data by scaling it\n",
    "adaptation_feats = [years, wordcount, downloads, detective, horror]\n",
    "scaler = StandardScaler()\n",
    "\n",
    "cols = []\n",
    "for col in adaptation_feats:\n",
    "    col = np.array(col).reshape(-1,1)\n",
    "    scaled_col = scaler.fit_transform(col)\n",
    "    cols.append(scaled_col)\n",
    "\n",
    "# convert our data into a feature matrix\n",
    "adaptation_X = np.hstack(cols)\n",
    "print(adaptation_X.shape)\n",
    "print(len(adaptation_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_8b7d5992_1635_11eb_91be_82185fe7d140row0_col0,#T_8b7d5992_1635_11eb_91be_82185fe7d140row0_col1,#T_8b7d5992_1635_11eb_91be_82185fe7d140row0_col2,#T_8b7d5992_1635_11eb_91be_82185fe7d140row0_col3,#T_8b7d5992_1635_11eb_91be_82185fe7d140row0_col4,#T_8b7d5992_1635_11eb_91be_82185fe7d140row0_col5{\n",
       "            background-color:  #006837;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_8b7d5992_1635_11eb_91be_82185fe7d140row1_col0,#T_8b7d5992_1635_11eb_91be_82185fe7d140row2_col0,#T_8b7d5992_1635_11eb_91be_82185fe7d140row3_col0,#T_8b7d5992_1635_11eb_91be_82185fe7d140row3_col1,#T_8b7d5992_1635_11eb_91be_82185fe7d140row4_col0,#T_8b7d5992_1635_11eb_91be_82185fe7d140row4_col2,#T_8b7d5992_1635_11eb_91be_82185fe7d140row4_col3,#T_8b7d5992_1635_11eb_91be_82185fe7d140row4_col4,#T_8b7d5992_1635_11eb_91be_82185fe7d140row4_col5{\n",
       "            background-color:  #a50026;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_8b7d5992_1635_11eb_91be_82185fe7d140row1_col1,#T_8b7d5992_1635_11eb_91be_82185fe7d140row5_col0{\n",
       "            background-color:  #ab0626;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_8b7d5992_1635_11eb_91be_82185fe7d140row1_col2,#T_8b7d5992_1635_11eb_91be_82185fe7d140row1_col5,#T_8b7d5992_1635_11eb_91be_82185fe7d140row3_col2,#T_8b7d5992_1635_11eb_91be_82185fe7d140row3_col5{\n",
       "            background-color:  #fed481;\n",
       "            color:  #000000;\n",
       "        }#T_8b7d5992_1635_11eb_91be_82185fe7d140row1_col3,#T_8b7d5992_1635_11eb_91be_82185fe7d140row3_col3{\n",
       "            background-color:  #f26841;\n",
       "            color:  #000000;\n",
       "        }#T_8b7d5992_1635_11eb_91be_82185fe7d140row1_col4,#T_8b7d5992_1635_11eb_91be_82185fe7d140row3_col4{\n",
       "            background-color:  #f88c51;\n",
       "            color:  #000000;\n",
       "        }#T_8b7d5992_1635_11eb_91be_82185fe7d140row2_col1{\n",
       "            background-color:  #be1827;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_8b7d5992_1635_11eb_91be_82185fe7d140row2_col2,#T_8b7d5992_1635_11eb_91be_82185fe7d140row2_col5{\n",
       "            background-color:  #dd3d2d;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_8b7d5992_1635_11eb_91be_82185fe7d140row2_col3{\n",
       "            background-color:  #db382b;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_8b7d5992_1635_11eb_91be_82185fe7d140row2_col4{\n",
       "            background-color:  #dc3b2c;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_8b7d5992_1635_11eb_91be_82185fe7d140row4_col1{\n",
       "            background-color:  #a70226;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_8b7d5992_1635_11eb_91be_82185fe7d140row5_col1{\n",
       "            background-color:  #ee613e;\n",
       "            color:  #000000;\n",
       "        }#T_8b7d5992_1635_11eb_91be_82185fe7d140row5_col2,#T_8b7d5992_1635_11eb_91be_82185fe7d140row5_col5{\n",
       "            background-color:  #fece7c;\n",
       "            color:  #000000;\n",
       "        }#T_8b7d5992_1635_11eb_91be_82185fe7d140row5_col3{\n",
       "            background-color:  #fff3ac;\n",
       "            color:  #000000;\n",
       "        }#T_8b7d5992_1635_11eb_91be_82185fe7d140row5_col4{\n",
       "            background-color:  #fee797;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >fit_time</th>        <th class=\"col_heading level0 col1\" >score_time</th>        <th class=\"col_heading level0 col2\" >test_accuracy</th>        <th class=\"col_heading level0 col3\" >test_f1</th>        <th class=\"col_heading level0 col4\" >test_f1_macro</th>        <th class=\"col_heading level0 col5\" >test_f1_micro</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140level0_row0\" class=\"row_heading level0 row0\" >random forest</th>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row0_col0\" class=\"data row0 col0\" >0.874644</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row0_col1\" class=\"data row0 col1\" >0.049430</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row0_col2\" class=\"data row0 col2\" >0.699385</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row0_col3\" class=\"data row0 col3\" >0.537544</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row0_col4\" class=\"data row0 col4\" >0.655631</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row0_col5\" class=\"data row0 col5\" >0.699385</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140level0_row1\" class=\"row_heading level0 row1\" >svm-linear</th>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row1_col0\" class=\"data row1 col0\" >0.002092</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row1_col1\" class=\"data row1 col1\" >0.005353</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row1_col2\" class=\"data row1 col2\" >0.660000</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row1_col3\" class=\"data row1 col3\" >0.389872</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row1_col4\" class=\"data row1 col4\" >0.576258</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row1_col5\" class=\"data row1 col5\" >0.660000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140level0_row2\" class=\"row_heading level0 row2\" >svm-rbf</th>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row2_col0\" class=\"data row2 col0\" >0.002011</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row2_col1\" class=\"data row2 col1\" >0.007034</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row2_col2\" class=\"data row2 col2\" >0.644000</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row2_col3\" class=\"data row2 col3\" >0.375110</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row2_col4\" class=\"data row2 col4\" >0.562883</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row2_col5\" class=\"data row2 col5\" >0.644000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140level0_row3\" class=\"row_heading level0 row3\" >svm-linear-c=0.8</th>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row3_col0\" class=\"data row3 col0\" >0.001652</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row3_col1\" class=\"data row3 col1\" >0.004731</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row3_col2\" class=\"data row3 col2\" >0.660000</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row3_col3\" class=\"data row3 col3\" >0.389872</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row3_col4\" class=\"data row3 col4\" >0.576258</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row3_col5\" class=\"data row3 col5\" >0.660000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140level0_row4\" class=\"row_heading level0 row4\" >svm-rbf-c=0.8</th>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row4_col0\" class=\"data row4 col0\" >0.002304</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row4_col1\" class=\"data row4 col1\" >0.004957</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row4_col2\" class=\"data row4 col2\" >0.636308</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row4_col3\" class=\"data row4 col3\" >0.354158</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row4_col4\" class=\"data row4 col4\" >0.550273</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row4_col5\" class=\"data row4 col5\" >0.636308</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140level0_row5\" class=\"row_heading level0 row5\" >logistic</th>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row5_col0\" class=\"data row5 col0\" >0.014166</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row5_col1\" class=\"data row5 col1\" >0.012821</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row5_col2\" class=\"data row5 col2\" >0.659385</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row5_col3\" class=\"data row5 col3\" >0.439137</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row5_col4\" class=\"data row5 col4\" >0.594725</td>\n",
       "                        <td id=\"T_8b7d5992_1635_11eb_91be_82185fe7d140row5_col5\" class=\"data row5 col5\" >0.659385</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f8b2758de10>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modelling\n",
    "adaptation_models = {\n",
    "    'random forest':RandomForestClassifier(n_estimators=300),\n",
    "    'svm-linear':svm.SVC(kernel='linear'),\n",
    "    'svm-rbf':svm.SVC(kernel='rbf'),\n",
    "    'svm-linear-c=0.8':svm.SVC(kernel='linear', C=0.8),\n",
    "    'svm-rbf-c=0.8':svm.SVC(kernel='rbf', C=0.8),\n",
    "    'logistic':LogisticRegression()\n",
    "}\n",
    "\n",
    "adaptation_scores = {}\n",
    "for name, model in adaptation_models.items():\n",
    "    score = cross_validate(model, adaptation_X, adaptation_y, cv=5, \n",
    "                                  scoring=['accuracy', 'f1', 'f1_macro', 'f1_micro'], return_estimator=True)\n",
    "    adaptation_models[name] = score['estimator'][np.argmax(score['test_accuracy'])]\n",
    "    del score['estimator']\n",
    "    adaptation_scores[name] = score\n",
    "compare_scores(adaptation_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloads 0.47909362785619897\n",
      "wordcount 0.2669241981305822\n",
      "years 0.2099144889887458\n",
      "detective 0.022595388454808057\n",
      "horror 0.021472296569665147\n"
     ]
    }
   ],
   "source": [
    "# Feature importance\n",
    "importance = adaptation_models['random forest'].feature_importances_\n",
    "indices = np.argsort(importance)[::-1]\n",
    "adaptation_feats_names = ['years','wordcount','downloads','detective','horror']\n",
    "for i,ind in enumerate(indices):\n",
    "    print(adaptation_feats_names[ind], importance[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Results and discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Responsibility and resources consulted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
